{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rWXgZorf762"
      },
      "source": [
        "# ReAct agent from scratch with Gemini 2.5 and LangGraph\n",
        "\n",
        "AI applications are evolving from simple chatbots to (semi-)autonomous systems capable of complex reasoning, planning, and interaction with the real world. We call these system agents.\n",
        "\n",
        "> An AI Agent is a system that uses LLMs to decide the control flow of an application.\n",
        "\n",
        "\n",
        "Agents are not just theoretical concepts; they are and will be deployed in production across various verticals, tackling increasingly more complex and longer-running tasks. In this blog post, we'll explore how to create a ReAct agent using Google's Gemini 2.5 Pro or Gemini 2.0 Flash and LangGraph from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnv3CKFaf764"
      },
      "source": [
        "## What are ReAct Agents?\n",
        "\n",
        "ReACT (Reasoning and Acting) Agents are AI systems that combine reasoning capabilities of LLMs with action execution, enabling them to iteratively think through problems, use tools, and act based on observations to achieve the user goals.\n",
        "\n",
        "The ReAct pattern was first introduced in the [“ReAct: Synergizing Reasoning and Acting in Language Models”](https://arxiv.org/abs/2210.03629) in 2023. It was inspired by how humans plan and solve complex tasks, rather than implementing predefined workflows. ReAct agents rely on the LLM's reasoning capabilities to dynamically adjust their actions based on new information or the results of previous steps.\n",
        "\n",
        "ReACT Agents have gained traction due to their ability to handle complex tasks by breaking them into manageable reasoning steps and leveraging external tools.\n",
        "\n",
        "\n",
        "The ReAct agent:\n",
        "\n",
        "1. Takes a user **query** as input\n",
        "2. Reasons about the query and decides on an action\n",
        "3. Executes the chosen action using available tools\n",
        "4. Observes the result of the action\n",
        "5. Repeats steps 2-4 until it can provide a final answer\n",
        "\n",
        "### The First ReAct Agents\n",
        "\n",
        "These first-generation ReAct agents used a simple but effective prompting technique to generate a chain of \"Thought, Action, Observation\" steps:\n",
        "\n",
        "- \"Thought\" component plans the next action or decide it knows the final answer\n",
        "- \"Action\" interacts with external resources (like search engines or calculators).\n",
        "- \"Observation\" incorporates the results from the action into the reasoning process.\n",
        "\n",
        " Here's a pseudo-code example demonstrating the flow of an early ReAct agent.\n",
        "\n",
        "```xml\n",
        "User: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
        "\n",
        "Thought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
        "Action: [search(\"Olivia Wilde boyfriend\")]\n",
        "Observation: Olivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\n",
        "\n",
        "Thought: I need to find out Harry Styles' age.\n",
        "Action: [search(\"Harry Styles age\")]\n",
        "Observation: 29 years\n",
        "\n",
        "Thought: I need to calculate 29 raised to the 0.23 power.\n",
        "Action: [calculator(29^0.23)]\n",
        "Observation: Answer: 2.169459462491557\n",
        "\n",
        "Thought: I now know the final answer.\n",
        "Final Answer: Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557.\n",
        "```\n",
        "\n",
        "### Current ReAct Agents\n",
        "\n",
        "Since the introduction of ReAct Agent the capabilities of LLMs has evolved. One of the most important improvements we made is function calling. **Function calling allows us to to connect LLMs to external tools in a structured way, which is more reliable than parsing raw text and reduces the likelihood of errors and hallucinations.**\n",
        "\n",
        "Here's a pseudo-code example demonstrating the flow of an ReAct agent using function calling\n",
        "\n",
        "```xml\n",
        "User: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n",
        "\n",
        "Assistant: FunctionCall(name=\"search\", args={\"query\": \"Olivia Wilde boyfriend\"})\n",
        "User: FunctionResponse(result=\"Olivia Wilde started dating Harry Styles after ending her years-long engagement to Jason Sudeikis — see their relationship timeline.\")\n",
        "\n",
        "Assistant: FunctionCall(name=\"search\", args={\"query\": \"Harry Styles age\"})\n",
        "User: FunctionResponse(result=\"29 years\")\n",
        "\n",
        "Assistant: FunctionCall(name=\"calculator\", args={\"expression\": \"29^0.23\"})\n",
        "User: FunctionResponse(result=\"2.169459462491557\")\n",
        "\n",
        "Assistant: Harry Styles, Olivia Wilde's boyfriend, is 29 years old. His age raised to the 0.23 power is 2.169459462491557.\n",
        "```\n",
        "\n",
        "### Traditional ReAct Agents vs. Current ReAct Agents (Function Calling)\n",
        "\n",
        "| **Aspect** | **Traditional ReAct Agents** | **Current ReAct Agents (Function Calling)** |\n",
        "| --- | --- | --- |\n",
        "| Actions | Text-based description, parsed by system | Direct function calls in structured format |\n",
        "| Efficiency | Lower, due to parsing errors | Higher, with reduced parsing overhead |\n",
        "| Reliability | More prone to errors and hallucinations | More reliable and accurate tool execution |\n",
        "| LLM Requirement | Works with any LLM | Requires LLMs supporting function calling |\n",
        "| Implementation | Primarily through careful prompt engineering | Often facilitated by SDKs and frameworks like LangGraph |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5bOvWcEf765"
      },
      "source": [
        "# How to create a ReAct agent from scratch with LangGraph\n",
        "\n",
        "We know have base understanding on how ReAct agents work. Now, let’s build our own from scratch. We are going to use LangGraph and Gemini 2.5 Pro. LangGraph is framework for building controllable agents. LangGraph comes already with prebuilt ReAct agent [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent), but sometimes you might want more control and customization.\n",
        "Additionally, It is good to understand the underlying concepts and how to build your own ReAct agent from scratch.\n",
        "\n",
        "LangGraph models agent as graphs. You define the behavior of agents using three key components:\n",
        "- `State`: Shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel, which is shared across all nodes.\n",
        "- `Nodes`: Encodes logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State, e.g. LLM calls, tool calls, etc.\n",
        "- `Edges`: Determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.\n",
        "\n",
        "First, we installed required packages and set our API keys. If you don't have an API Key yet you can get one for free in the [Google AI Studio](https://aistudio.google.com/app/apikey)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vovp72kdf765",
        "outputId": "587e4a39-bade-4e35-ac34-f4e6604f6ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.21-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.49)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.23-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.7-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.60-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.0)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.19)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.0)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.3.21-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.23-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.7-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.60-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, google-ai-generativelanguage, langgraph-prebuilt, langchain-google-genai, langgraph\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.17 langchain-google-genai-2.1.2 langgraph-0.3.21 langgraph-checkpoint-2.0.23 langgraph-prebuilt-0.1.7 langgraph-sdk-0.1.60 ormsgpack-1.9.1 xxhash-3.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "617586ee855747ec84301fc113cfb5c2",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install langgraph langchain-google-genai geopy requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFU8vZ-yf766"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Read your API key from the environment variable or set it manually\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\", \"place_your_key_here\") # "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSpNdyFxf767"
      },
      "source": [
        "We are going to create the most basic ReAct agents, which uses mocked 1 tool to retrieve the weather for a given location. For this we need to store the conversation history as a list of messages in our graph state. We are going to use the [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) helper function to add messages to the state. The `add_messages` function is a [reducer](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) that merges two lists of messages, updating existing messages by ID and ensures the state is \"append-only\", unless the new message has the same ID as an existing message. For demonstration purposes we also store the number of steps in the state.\n",
        "\n",
        "Note: Since having a list of messages in the state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nd_-arg7f767"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Sequence, TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages # helper function to add messages to the state\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"The state of the agent.\"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    number_of_steps: int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbSZjfShf767"
      },
      "source": [
        "Next, we define our weather tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "faUclL7Lf767"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from geopy.geocoders import Nominatim\n",
        "from pydantic import BaseModel, Field\n",
        "import requests\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"weather-app\")\n",
        "\n",
        "class SearchInput(BaseModel):\n",
        "    location:str = Field(description=\"The city and state, e.g., San Francisco\")\n",
        "    date:str = Field(description=\"the forecasting date for when to get the weather format (yyyy-mm-dd)\")\n",
        "\n",
        "@tool(\"get_weather_forecast\", args_schema=SearchInput, return_direct=True)\n",
        "def get_weather_forecast(location: str, date: str):\n",
        "    \"\"\"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\"\"\"\n",
        "    location = geolocator.geocode(location)\n",
        "    if location:\n",
        "        try:\n",
        "            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n",
        "            data = response.json()\n",
        "            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "    else:\n",
        "        return {\"error\": \"Location not found\"}\n",
        "\n",
        "tools = [get_weather_forecast]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOpj1LdBf767"
      },
      "source": [
        "Next, we initalize our model and bind the tools to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7rt-6_Nf768",
        "outputId": "00e30fa0-34db-478d-c24b-55549f986bd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather_forecast', 'arguments': '{\"date\": \"2025-04-01\", \"location\": \"Beijing\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro-exp-03-25', 'safety_ratings': []}, id='run-90cfed77-c07b-4d37-a12e-f063abfd6f79-0', tool_calls=[{'name': 'get_weather_forecast', 'args': {'date': '2025-04-01', 'location': 'Beijing'}, 'id': '5dd592a4-0aa5-4f8f-80a4-1f15baac7241', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 31, 'total_tokens': 162, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Create LLM class\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model= \"gemini-2.5-pro-exp-03-25\", # replace with \"gemini-2.0-flash\"\n",
        "    temperature=1.0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    google_api_key=api_key,\n",
        ")\n",
        "\n",
        "# Bind tools to the model\n",
        "model = llm.bind_tools([get_weather_forecast])\n",
        "\n",
        "# Test the model with tools\n",
        "model.invoke(\"What is the weather in Beijing on 1st of April 2025?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yql0qMDbf768"
      },
      "source": [
        "The last step before we can run our agent is to define our nodes and edges. In our examples we have **two nodes and 1 edge**.\n",
        "- `call_tool` **node** that executes our tool method. LangGraph has a prebuilt node for this called [ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/).\n",
        "- `call_model` **node** that uses the `model_with_tools` to call the model.\n",
        "- `should_continue` **edge** that decides whether to call the tool or the model.\n",
        "\n",
        "**The number of nodes and edges is not fixed. You can add as many nodes and edges as you want to your graph.** For example, you could add a node for adding structured output or self-verification/reflection node to check the model output before calling the tool or the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nDIwI8vBf768"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.messages import ToolMessage, SystemMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "# this is similar to customizing the create_react_agent with 'prompt' parameter, but is more flexible\n",
        "# system_prompt = SystemMessage(\n",
        "#     \"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-04-01. Help the user with their questions. Use the history to answer the question.\"\n",
        "# )\n",
        "\n",
        "# Define our tool node\n",
        "def call_tool(state: AgentState):\n",
        "    outputs = []\n",
        "    # Iterate over the tool calls in the last message\n",
        "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
        "        # Get the tool by name\n",
        "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
        "        outputs.append(\n",
        "            ToolMessage(\n",
        "                content=tool_result,\n",
        "                name=tool_call[\"name\"],\n",
        "                tool_call_id=tool_call[\"id\"],\n",
        "            )\n",
        "        )\n",
        "    return {\"messages\": outputs}\n",
        "\n",
        "def call_model(\n",
        "    state: AgentState,\n",
        "    config: RunnableConfig,\n",
        "):\n",
        "    # Invoke the model with the system prompt and the messages\n",
        "    response = model.invoke(state[\"messages\"], config)\n",
        "    # We return a list, because this will get added to the existing messages state using the add_messages reducer\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# Define the conditional edge that determines whether to continue or not\n",
        "def should_continue(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    # If the last message is not a tool call, then we finish\n",
        "    if not messages[-1].tool_calls:\n",
        "        return \"end\"\n",
        "    # default to continue\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk4Chuggf768"
      },
      "source": [
        "Awesome, now we have all the components to build our agent. Let's put them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wFJ9llmhf768"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define a new graph with our state\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# 1. Add our nodes\n",
        "workflow.add_node(\"llm\", call_model)\n",
        "workflow.add_node(\"tools\",  call_tool)\n",
        "# 2. Set the entrypoint as `agent`, this is the first node called\n",
        "workflow.set_entry_point(\"llm\")\n",
        "# 3. Add a conditional edge after the `llm` node is called.\n",
        "workflow.add_conditional_edges(\n",
        "    # Edge is used after the `llm` node is called.\n",
        "    \"llm\",\n",
        "    # The function that will determine which node is called next.\n",
        "    should_continue,\n",
        "    # Mapping for where to go next, keys are strings from the function return, and the values are other nodes.\n",
        "    # END is a special node marking that the graph is finish.\n",
        "    {\n",
        "        # If `tools`, then we call the tool node.\n",
        "        \"continue\": \"tools\",\n",
        "        # Otherwise we finish.\n",
        "        \"end\": END,\n",
        "    },\n",
        ")\n",
        "# 4. Add a normal edge after `tools` is called, `llm` node is called next.\n",
        "workflow.add_edge(\"tools\", \"llm\")\n",
        "\n",
        "# Now we can compile and visualize our graph\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzsMw9hXf768"
      },
      "source": [
        "We can visualize our graph using the `draw_mermaid_png` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "6wTNJjKpf769",
        "outputId": "32339b10-8ada-46a0-96e4-10c6b46b74bc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAERCAIAAADHRs0RAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXdYVFce98/0PsPQh14EAUERbLFEExQVe8u6lpU16lqSaBIxBvU1ZteQZo36rgkqiWjsRiMGjSSamJjEGgUEBEHqMAxtep/3j7vvSHRAhDtz7lzO58mTB24558vcr2d+95TfoVitVoBAkBcqbAEIhGNBFkeQHGRxBMlBFkeQHGRxBMlBFkeQHDpsAXCQVug1CpNaYTKbrHqtBbacZ8NkU2l0Ck9I4wkZPiEsCgW2INeB0qP6xYtuKMvvqR7mq0NjeIACeEK62IfhEhZncWjNMoNGYTLorVUlmpBobmgsP2aIEHn9mfQUi9+72vp7bmNwNC8slhcax6e6eIBWUaAuz1c/uq+JGyFKTBLDlkNoyG9xWZX+u6y6kBje0EmeDCbZGr1fv23M/7V13ALfoCgubC0EheQWL/xdce9q64RXJXw30r516LWWvCMy32BWwsuoObcDmS1e9qeq4r4mabY3bCHO4Jezcr4bvd+LbrCFEA7SWvz6xaYWmXHMPB/YQpzHz9/ILWbryBlesIUQCxd/7WqH8ny1rFLfo/wNABgx1dNithZcU8AWQixIaHFFo+n+H4oJiySwhUDgpVe868q19Y/0sIUQCBJa/Oo3DVEDhbBVQCN2qOin0w2wVRAIsllcWqFTK81hcTzYQqDhG8LmCWgP76lhCyEKZLN44TXFiKk9/X1r+FTP4ptK2CqIAqksrtNYyvJVviEsZ1Z67Nix9957rws3vvPOO99++60DFAGhB6NJqm+uNziicJeDVBYvv6cKi+U7udL79+87+cbOENqH/zAfxSqAbP3iPxyV9erHd9BQ9u3bt3fv3l1aWmo2myMjI1esWJGQkLBkyZJbt25hFxw6dKh37965ubkHDx6srKxkMpl9+/Z9++23AwICsDabQqGEhIRkZ2dnZGS8+eab2F18Pv/y5cu4q62v1N/+sXncAl/cS3Y5SNWK15VrBWKHDNRrtdpVq1aFhYUdOHDgyy+/jIiIeOONNxQKxdatW6OiopKTky9dutSrV6+CgoL169cPGzbs4MGDO3fu1Gq1aWlpWAkMBqO0tLSoqGjnzp1xcXHnz58HAKSlpZ05c8YRgoXu9OoHWkeU7HKQauaGRmHmChzyF0mlUrVanZKSEhoaCgBYvXr1mDFjmEwmm82m0+lMJtPNzQ0AEBwcfPDgwYiICDqdDgCYM2fOW2+91dTU5O7uDgCorq7et2+fSCQCAOj1egAAl8vFfsUdDp+m15otFuDqcyq7D3ksbjFbjQYLi+uQRxoUFBQcHLx+/fqZM2cOGTKkd+/eiYmJT1/G5/Nramp27dpVVVWl0+mMRiMAQKFQYBYPDg52kKHtwhPSNQoTieefdRLy/Bu3mAGH76jHSaPRMjMzR48effr06Xnz5k2aNCknJ+fpyy5evLh27drY2NidO3cePnx43bp1bc/y+U59FWZzqRazMyskKOSxOJ1JMegsBp2jlvCIxeJVq1adOXPm2LFjgwYN2rhx49NdIqdPnx4wYMCyZctCQkI8PT11Op2DxHSGZpmRJ6RBFEAQyGNxAABXQNMoHdJw1dTU2Po9wsLC0tPTqVRqWVkZdsTWK2UwGLCgHCM3N7ft2adxXHeWXmuh0Sk0BtmWgHQBUlncvxdHq3KIxaVS6Zo1a7KzsysqKh49epSZmUmlUuPi4gAAAoGguLi4uLi4paUlNjb2t99+y8/Pr6ury8jI8PT0BAAUFhY+3ZyzWCwWi3Xr1q3i4mKTyYS7YE2rORitAwIAAEDr2sgcMVE0GuvKdcHR+D9aPz8/Pz+/kydPZmVlnTlzRqPRrF27tm/fvgAAkUiUk5Nz6tSp/v37JycnP3jw4PPPPz9//nxiYuKbb7559+7do0ePhoSEVFZWqlSqKVOm2Mq0WCynT5++cOHCzJkzWSycR2QLrimYbGpABHI5uYZ+FE2m07uqF/yfENhC4HN8W9WL0718gtmwhcCHVIGK0J3uE8RurjfCFgIZvcbC4tCQvzHI1mkamSi4dk6e8mq76yEWL1784MGDp4+bzWasc9DuXWfOnHFQl/adO3dWrVpl95TZbG5PDwAgLy+vvbPXchp78nTiJyBVoIJxfHv1iKmeviH227CGhgZsROYJsOHG9mJiX19fqmPGCfV6fWNjY3unGAxGe/X6+fnZPY6itScgocXrHuruX1e8/LcesfD+aX45I5eEcVArboNUsTiGJIzt7sO8ekYOWwgEbv3QDCgA+bstJLQ4ACB+lJtObb6V1wxbiFMpvq6sLtEOm+wJWwixIGGgYuP33CYGg5qQ1COy59z/Q1n7UNtD8iI9F2S2OADg6jdyncY8eg7JE6pcy2lStxhHzyX5n9k1SG5xAEDRH8qfzzS8kOIRO8x5E1mdRvEN5a/n5P1HieNH9Ygvqy5AfosDAIx6yy9nG6tKNH1eEIbG8sXeDNiKuouiyVSer3p4V813ow+d5METkW18A0d6hMUxlE2me7+0PrynAgCE9OHTGYAroAvd6SaTC3wCDAZF0WTSKM16raWmVGPQWUJjeX1eEHlImLClEZ0eZHEbzTJjfYVO1WpSK0xUGkXVgvNEv+vXryckJHQwMNkF+CK6xWzlCGh8Ed07iO3ph5zdWXqixR3NiBEjLly4wOWiWX6EgJz94giEDWRxBMlBFsefmJgY2BIQj0EWx5/CwkLYEhCPQRbHH7FYTEH7YRIGZHH8aW5uRv1UxAFZHH/8/f1hS0A8Blkcf2pqamBLQDwGWRx/+vbti2Jx4oAsjj93795FsThxQBZHkBxkcfzx8PBAgQpxQBbHn8bGRhSoEAdkcfzx9vZGrThxQBbHH5lMhlpx4oAsjiA5yOL407t3b9gSEI9BFsef4uJi2BIQj0EWR5AcZHH8iY2NhS0B8RhkcfzJz8+HLQHxGGRxBMlBFscfNNOQUCCL4w+aaUgokMURJAdZHH9QkglCgSyOPyjJBKFAFkeQHGRx/EF5VAgFsjj+oDwqhAJZHH/QTENCgSyOP2imIaFAFkeQHGRx/JFIJOh1kzggi+NPXV0det0kDsji+BMXFwdbAuIxyOL4c+/ePdgSEI9BFsefuLg4FIsTB2Rx/Ll37x6KxYkDsjj+BAUFwZaAeAzaWhY3xo8fz2QysWxY7u7udDrdbDb7+Pjs27cPtrQeDR22APJAo9Fs+0PU19cDALhc7urVq2Hr6umgQAU3+vXr98RXYnh4+KhRo+ApQgBkcTyZPXu2RCKx/crhcFJTU6EqQgBkcTyJi4uLi4uzNeQREREjR46ELQqBLI4rs2fP9vb2BgCIRKL58+fDloMAyOI4069fv5iYGKvVGhER8dJLL8GWgwCoR8UeVtBUb2iRGy3mrnSnpoxcKH9EnzBqZumfqi7cTqVShO4Md18GlYbGR/EB9Yv/hQd3VHd/btUqzf69OCqF2fkCuHx6/SMNg0mNGSLs84LQ+QLIB2rFH1N6R11wTTFmnj8FevhmBVdP11tMIG4Ecnl3gf4wiUJ5gebu1dakOX7w/Q0AoIDh032qHmgKf1fAluLyEOF5EoI/r7QMm+INW8VfeGGST8GvCqsFtg4XB1kcAACMBqu0QssVEitsozMpGqVJ2WyELcS1QRYHAABFo9EnmANbhR28A9mtcmTxboEsDgAAFArQKE2wVdhBq0ZhSndBFkeQHGRxBMlBFkeQHGRxBMlBFkeQHGRxBMlBFkeQHGRxBMlBFkeQHGRxBMlBFkeQHGTxLrLxvTVvr16G/TxlWtJXBzNhK0LYB1kcQXKQxREkB1kcT86cPTF1+ujbd268unj2+AnDX108u7S05MKFc/P+MW3CpBffefeNlpZm2Bp7HMjieEKn09Vq1blzp7Zv++LY0e+MRuPG99Ju37mR+fnXWftPFBcXHjueDVtjjwNZHGdMJtPf/vYPAV8g4AsGDxpWW1ez9F8r2Wy2l5d3//gBpaVoS05ngyyOP4EBwdgPPB5PKBS5uYmxX7lcnkrdlfxBiO6ALI4/DAbD9jOWVB8BEWRxBMlBFkeQHGRxBMlBFkeQHJSZFgAAmqSG77Kkk5cRbjPB7w/WDhzjFtibC1uIC4NacQTJQRZHkBxkcQTJQRZHkBxkcQTJQRZHkBxkcQTJQRZHkBxkcQTJQRZHkBxkcQTJQRZHkBxkcUJjMVvUajVsFa4NsjgAAFCpFKE7oxMXOhu+G339hvT6+nrYQlwYZHEAAHDzZlSXakxGws0rrihQf33685qaGqMR7b7ZRZDFAQBALpd7BOvqK3SwhfyFhmp9WByfwaQkJCRQKJS5c+caDAbYolwPZHGg0+nmzp07c1n0tXP1ikaibDBr0Fl+OiF96RUv7Fc6nb5hw4bMTJQc9Lnp6at+ZDKZXq8PDAwEAJiM1uwPHsUOc+eJ6G7eTKsFwidDpVJa5AZ1q+nGRfmCDSFsnp02KDs7e968ec7X5qL0aIvv27dv6NCh0dHRbQ/eymuuKdMCCqVZ2sWooKW5WeTmRqFQunCv0INOoVL8wtgDk93bu+bkyZPl5eWrV6/umrweh7WnUlNTs3v3btyLPXTo0IABA7Zu3Yp7yW0pKyuzWq0PHjxwaC3koIfG4vfu3eNyucuXL8e3WJPJdPLkSavVevny5aamJnwLb0tYWBgAIDc3Ny8vz3G1kIOeaPFJkyYFBga6ubnhXvLRo0dramoAALW1tUeOHMG9/Cd47bXXzGazo2txdXqWxZVKZX5+/t69ex3hb6PRePLkSZPJhIV/eXl5Dm3IMZKTkwEA8+bNq6iocHRdLkoPsvjt27cvXrwYGxvr5+fniPLPnj2LNeEYlZWVhw8fdkRFT3PgwIHsbJS53D49yOJZWVkzZsxwXPlHjx5tGzZYrdYffvihsbHRcTXaYDAY69evBwCcP3/eCdW5Fj3C4tevXwcA7Nixw6G1VFdX237GumIrKyu/+uorh1b6BBERERMnTsSCJQQG+fvFN2/ePHbs2AEDBjitxhEjRly4cIHLhZOlTSqVKpVKLy8vR7xvuCIkb8WVSmV0dLQz/Q0A8PHx6dq4Dy74+vpGRETIZLIvvvgClgZCQWaL//bbbywWa/r06U6uVyqVQrQ4RmRkpNlsLi8vhyuDCJDW4tOmTYuOjoayDwlBYr+lS5e6u7sXFhZqNBrYWmBCTourVKodO3aIRCIotUskEhqNBqXqJxCJRL169Ro7dqxCoYCtBRoktHhmZiaPxwsKgpYs/NGjRwSxOLaf1s8//1xaWtpj23KyWXzUqFGpqakQQ2Gr1crn86lUYn2wCQkJBoNh165dsIVAgFhPovtcvnyZTqdDFKDT6Yi5CM3NzY3P51+5cgW2EGdDEotbrdb3338ftgoAAFCr1Xw+H7YK+6SmpoaGhsJW4WxIYvEZM2Zs2LABtgqAvenyeDzYKtoFe0UZN25czxkBJYnFT506Bb0rGkOlUkkkEtgqnsGxY8eysrJgq3ASLm/xzZs3t7a2wlbxGLlczmKxYKt4BkKhcNGiRdjSENhaHI5rWzw9PX3u3Lmw+r/t0tTU5O7e7rJLorFly5a2E4BJCfmnYTmZI0eOGI3G+fPnwxbSWS5dujRy5EgGg4jJwHDBVVvxs2fP/vLLL7BV2KG4uJhQ3yrPZPTo0QaDgcQTzV3S4t99911dXd2wYcNgC7FDTU2Nv78/bBXPB4/Hu3btWlVVFWwhDgEFKjiTkpJy4MABHx8f2EKem/z8/NjYWNgq8KfdgUClUkmQbri2mM3mmzdvDho06HlvdM5wjNFoDAoKckV/AwBiY2PPnj0bEhKCZbBwDk54Lu1aXKfTEbCBb2lpiY6O7sKMIudYvKioSKcjVu7P52Ly5Mnbt28Xi8VO6/fk8XiObkldLBZ3c3MjziS+pykqKoqKioKtolusWrUK1pI8B+EyFrdYLMSc3tSW4uJiV7c4hkajsVgssFXgg8tYvLm5Ge4Uws6g0+liYmJgq8ABLperUCgIGKl2AfgWLy8vT0lJKSgo6OAas9ksFosJ+PrbFpVKdfXq1cjISNhC8MGtq8l17fLzzz+npKRAmWoBx+IVFRWpqanYz56enitWrOh46hKNRiPaIoOnuXHjhpOX+jsai8Wi1Wphq+gucHxTWlpq+1kgEEyYMKGDeR0tLS0ukZzy+vXrJLM4lUqlUqkqlQq2kG7xHNFtUVHRvn37SktLBQLByJEj58+fj61vLygoyMrKwlwbFRWVmprau3dvAEBGRgYAIDEx8fjx442NjQEBAcuXL4+KisrOzsaS/aWkpCxZsqRfv34rVqz45JNP+vTp8/QtS5YsCQ8Pp9Fo7733HgAA+z8A4Icffvj0009PnjzJ4XBMJtORI0d++uknmUzm6ek5bdq0CRMmOOoDax+ZTOb8hBaOhsVisVis0tJS7BEbjcb4+PglS5Zgff85OTnZ2dkbN27cu3dvVVWVQCCYPXv22LFjsTzUn3/++Y8//mixWAYNGtSvXz9Yf0JnW3GpVLpu3TqJRJKRkbF06dJLly5h+85UV1evW7fO09Nz69atW7duZbPZ6enpDQ0NWHRRUFBQXFy8c+fOw4cPC4XCbdu2AQBmzpw5ZcoULy+vr7/+evz48W1refqWzz77jMPhdKxt3759p06deuWVV/bs2TNt2rS9e/fm5uZ24zPpClVVVQ8ePAgPD3dyvU5AJpOtXbuWQqF8+OGHGRkZSqUyPT0d21iLRqOp1eojR46kp6cfP348KSlp9+7dcrkcAHD8+PHc3NzFixd/9tlnsbGxTkhF3R6dtXhubi6TyVy5cmVUVNTQoUMXLVqEdeHl5ORwOJy33347NDQ0NDR0zZo1ZrPZltddp9MtXryYw+Gw2eyXXnqpqqpKp9Ox2Wwmk0mhUEQi0dNDDG1vGTlyJHZLB8LUanVOTs706dNHjx7t5+c3YcKEpKSk48ePd/UD6SKXLl0aPXq0kyt1DufPn6dQKEuWLAkJCYmMjFy9erVUKrXNgTOZTLNmzfLy8qJQKMnJySaT6eHDhwCAvLy8F154ITk5GXso/fv3h6W/sxYvLS3t1auXbdglKSlp5cqV2PHw8HBbdx6Hw/H398f+SACAn58fm83GfsbGF58Z2LW9BZvh2fEtDx8+NJlMCQkJtiN9+/atq6tz8nsSiS1eXFwcGRkZGBiI9SF6e3v7+vqWlZXZLrCtBxUIBFijYzQaa2tr23YuYbErFDobi6tUKi8vr6ePazSaJ94UuVyubYD96WRUz+xqtd1itVoxi3d8C1YX9k3atorm5uZnRjh4UV1drVKpyDHo8zRqtbqsrGzKlCm2I0ajse3uAE88ZavVin3xtj3utGfxNJ21uEgksjszhMfjPbFJu1qtxmXZC4VC6WCsx7bJKrYWOC0tLSQkpO0Fnp6e3dfQSc6fPz9p0iSnVedkuFxunz59Xn/9davV2traiiW87diyWPzZ1hhPmMSZdDZQCQsLKy4u1uv12K95eXlpaWkWiyUiIgJ70caOq1Sq6upqXIY/VCpV2/aby+W2jVhssVBoaCiDwWhpaQn8/wgEAqFQ6MxshseOHZs5c6bTqnMyUVFRtbW1EokkKCgoOjra19eXQqF03IoxmUwfH5+2SUNv377tFLF26KzFx48fbzabP/nkk8LCwmvXru3fvz8wMJBKpU6cOFGv12/fvr26urqiouLjjz/m8XhJSUkdl8bj8ZqamvLz8+vr6+1eYLFYDAZD29G1Xr16lZSUlJeXW63WGzdu3Lx501bU+PHjDx06dOXKlbq6uj///HPdunVY141zuHTpUmJiIomTeY8fP16r1W7durWsrKy+vv7EiRPLli0rKSnp+K6RI0deu3YtNze3vLz81KlTbWN3J9PZQMXb2/v999/fv39/enq6QCAYMWIENjwpkUj+85//HDhw4LXXXqNSqVjf9jOf96hRo/Ly8tLT02fNmjV8+PCnL6BQKE8UkpKSUlpaumbNGhqNlpCQkJqampGRgU0VWrRoEY/HO3DgQFNTk1gsHjx48IIFC57nQ+gWJ06cwFazkxUfH58PP/xw//79aWlpVCo1KCgoLS3tmS8ec+bMaW1tzczMxPrFFy5c+MEHH0CZ2tXuqp+GhgZyzMLB8Pb2dkSxjx492rNnz0cffeSIwmHR2NjY8XCyQqHgcrm4zIrDehu7X04HEHTih0qlIv7UWQDA3r17X375ZdgqnI1QKCT+rE8bBLW4yWQi/ryrqqqqwsJCbLy6p2EymVzlS56gNhIKhURe3YORmZlJ7ii8AywWi6tMzyKoxYnfhMtkstra2okTJ8IWAgcmk0mj0VyiISeokwiVptAuO3bscOhGtcSHy+USfJEKBkEtTvBQr6ioqKKiYty4cbCFwMRqtbrEgol234vFYrFzlfwFqVRK5MX227dvX7VqFWwVjkIsFneyA3v16tWvvvpqdHR0l+tywvdAuxaH2yvUt29fiLV3zC+//MJkMgcOHAhbiKPAFvt05sply5a1trYSvAORoAnftm3blpSUREyjz5kz58MPP4S4IxziuSBoLG6xWPLz82GrsMPhw4cTExORv2188cUX7U00IggEbcXVarXJZCJaFmODwYDNLoIthEBs2bJFIpHMmTMHtpB2IajFicmmTZv69+8/efJk2EIIRGNjY319PZETJBE0UMF20LOteyAC+fn5CoUC+fsJPDw8iOxvQlvc3d2dUCHB+vXrSdxR2B2WLl1K5HCcuN09mzZtIs5kw8zMzHHjxgUGBsIWQkQEAkFBQQFhs6qjWPzZ1NbW/utf//r2229hCyEozc3NVquVsPvUETdQwZbzQFzzZ2PDhg0ffPABbBXERSwWE9bfRLf41KlTL1++DFfD4cOHY2Ji4uLi4MogMhUVFURuAlCg0hEymWzBggXfffcdbCGERi6Xz50798KFC7CF2IfoFm9paWEymbC25ti0adOMGTNIuY8Zvty5cyc+Ph62CvsQOlABANTX18NaWZOVleXu7o783RkI628XsHjv3r3Hjh3r/F1Py8vLz5079/rrrzu5Xhdl+fLlFRUVsFXYh+gWBwAsWLCgbYf0kiVLnFDpnj17tm/f7oSKyIFOpyPsQi0XsDgA4OOPP25sbJw6derAgQNlMpmjq9u6dWt8fHxAQICjKyINmzdvjoiIgK3CPq5h8ZycnOTk5OrqaqvV6uilQDdu3CgpKZk7d65DayEZEomEsLt1EncAH2P48OEajYZKpdpWQAmFQofWuGXLlv379zu0CvKRlZUVHx9PzJdOorfi3t7ebRdZ2ZKOO4i1a9cuXLgQYjJsF6WoqAjb/YaAEN3iH3zwQVhYWNvOe8etFDx37hyLxRozZoyDyicxK1euHDJkCGwV9iG6xaOior788suBAwdiWdkpFIqDYr7W1tatW7du2rTJEYWTHolEgu2CQkCIbnFsQ4L//ve/EyZMwNa5YXsG4c7KlSt37NjhiJJ7Ahs2bLh48SJsFfaB/LppMQNFk7EzuTRWLEmTeIafPXuWy/BsleM8j/zYsWOD+o8KkkTZK5nCE9HoDBdI+wQRs9lM2Jkg0OaoVBRq7lxuqSnTeAWwtcrObo5sMZup+HcaWi0Wa3uZQxhMamujwdOf1e9Ft8gEh3yBuC6JiYlW6+OPzmKxUKlUPz+/s2fPwpb2GDiteMlNdf611hcmevPFRO+1xFC1mG5elOvUlr4jHNtl6VoMHTr0119/tf1KpVLZbPb8+fOhinoSCLF40Q3l/euKMfP9XMXfAAC+G33kK741Zdo7V1pgayEQ8+fPf2JnPH9//7bbFxIBZ1vcYgYF1xQv/13i5HpxYfg0n0f3NVo1hA1riMmgQYOio6NtsS6TyZw5c6Yz98rrDM62eGOd3qB1YYuYTVZ5dUcblvc02jbkAQEB06dPh63oSZxt8Va5URJK0MkMncEnmNPaSJS8AEQgMTER272NxWLNmjWLgMmEnW1xs8mqVZmcXCmO6LUWk4GgvWOwSE1N9fDw8Pf3nzZtGmwtdnCZFz4ELlSXaJvqDcoWs7rVbDJacOoyloyOe0csFl/4So5HaYDNpVMogCeiCcQ0n0C2h1+3gntk8R7Bw3vqohvKigK12J9vsQAGk0ZnMah03MazImISAQB4BXAmHdWkM8mkJpNBb1C3AKslvC8/ZrDAK4DVhdKQxUlORaHm59NyvieHyuRGjfKk0lxvmNagNTU2aPKON/EElFHTPQXuz2daZHEyk/uVTF5n9IrwYguI1ZH3XDA5dPcgIQDCVqn66LbquGGiweOeY5ceF5iGhegCaoX583cfWuj8gL6+Lu3vtoh8eb2GBtZWWs/srev8XcjiJESjtmR/UBk+JJAr7krwSnDEgSIKi3d8R2ddjixONjRK08F/V/QeGURjkvbhCn14bLHg0IedSj1C2k+hx5KdURX+AvlzBwi8OFxPQe6Xz85rjixOKi4dlvn18aYzCTfE6AjE/gKNln7/D0XHlyGLk4eaUm3dIwPfnQ1biPNwCxBdPvGMZdHI4uTh59Nyj1Di5vl2BFQaxStE9MeFpo6ucaIeaJw6fTRpzCDYKhxLZZGGzmNxRQTtQvkzP2/1hsFqNf6z7T1DxWV3NaD9iQguYPHy8rLZcybCVkF0Sm6pqAyS9H8/LxYrtaJQ3d5ZF7B4Scl92BJcgPICtdDbhWcpdweOmFv6Z7sWJ/oA/slTR3bt/hQA8FLSgBXL35o5Y869e3e+2LerpOQ+hUKJjopdvPj16Kg+2MUdnLJx9+7tzP27y8tLzWZzeHjkooUr+vVLgPGX4Ul9pV4s4TquI6W6tuj893uqa4vMJmNE+MDJ4990F0sAAL/+cfJC3ucL5205c36rrKGCyxUljfzn4MTJAACz2XTm/LZbd3OtFktM7+G9wgY4SBsAQOTNba7QtHeW6K34hJSp06fP9vb2+ebUpUkTZ1RVPVq9ZrmXp/fuz7J27TzA4XJXpy0TFuTcAAAHFklEQVSTyeoBAB2csqHVatPXrwoJDtu188CeXV+Gh0WsTX9DoXxGrxPxUTYZ9RpHraVqbpH+d/9yKoW6bOGepQt3azSKvVmvGU0GAACNStfpVJeu7P/H7Ix/r8tLjE859e1HLa0yAMAPP335+41vJo9f9ebyr0JD4i9dcWCaSBqTVl+psZjtx+NEtzibzWYxWRQKRSRyY7FYZ86e4HC47659Pzw8Ijw8Yt27/zGZTBcungMAdHDKhkwmVavVY0anBAeHhoSEvbZidcbmHUzXD2E1SjOV4agm/Nr1U4BCmTvr3xKfXoH+MX+f+V5Tc829gh+ws2aL6aUR/3AT+VAolEEJk8xmU630AQDg5p/fxcaMHJQwydMjcOigGZHhgx0kD4PJoatb7acqIbrFn6Dkwf3IiChbWkMulxsYGFxWVtLxKRsBAUGBgcGbM9Yf/jqr5EERjUaLj09ks12+I1mjNNNZjoo5K6vyg/xjOJz/5XMTu/m6i/1r6h5/sH4+/8sszuUIAQA6ndJkMsobqwL9H28cHhTwZMSILxweQ9NONh6ix+JPoNGoPdz/ktWAy+VpNOqOT9mg0Wg7t2d+feTLnJzTX2Tu8vHxXZi6LDl5grPkOw5rB71m3USrU9dKi995b7jtiNlsVCgfL/BhMP7SU2m1Wg0GLQCAQX98nMVy7Kuw2WQB7cyEdzGL83h8tVrV9oharcKc3cGptri5iZctXbVs6aqKiofHjmdnfLQxOCSsd2S0U+Q7Cp6QbjbqHVQ4m80LDYqfOWVt24NMZkeWZTDZAACt/vHj0GqVDpKHYdCZeEL7oZqLBSq9I2OKS+4bjf9bQqVUKSsrK6Ki+nR8ykZtXc3Vq//bqzYkJOytN9OpVGpFeZnT/w6c4QrpZqOjFn0HB8bKm6o83AO8vUKw/wCgCAVPth1tYdCZYjdJnfSB7UhJ2R8Okoeh15p5QvvttQtYnM8XNDbK7969LZXWTZkyS6/Xffzp+1VVjx4+LP3P5nU8Hn9s8kQAQAenbMjqpRs3rTl2PLuysqKq6tHB7EwqlRoT4/I7I4s8GAyHJRYdMmCaXq85cur9mtriBnnl9z/u+3TX36tqCjq+q39ccn7hld9ufFMnLb3yy6HaupKOr+8ORr3ZJ5hDacfLLmDxpJfH+fkFvJ227LvcM/5+AZ98tFsqrV205O+vvfFPYLVu27LXzU0MAOjglI34+MR30jZe/D7nX8vmLVvxjxs3f//3pk8DA4Ph/XH44OnPVDXrjVqHNOTuYsnShXuUqsbdmUt2/De1+MFv/5z7aXDgM9qFMS8vGtB/wrncnZ99saiyunBC8msAAIvVIT2binq1t3+73WLOzkxbfEP58J5m+HQfZ1aKI9cvyN296fGj3GALeZIrJxrkcrpHcE/MKlr1p3TUdPfASPvb17hAK47oDJEJfIuxJ6bpslgAk0lpz9+u16OCaA9JGIcKmlRyLd/T/sOWN1Vv/78L7J6iAEp7nY5DEqdOHIfnFtLrNyfZPW6xmIHVSqXZMWTvXkPm/21zewU2lDZGJXbUvYMsTh5enO55bl8939Pf7lmxyPet5QftntJolVyO/Z16WCwerhpBexqMRr0VACbDzmRgBqPdsTmjzqxsUPcb6d1Bjcji5MHLnxUex1PKNTxPO60ajUZ3F/vZvdH9ObKSdJf2NHQNZX3ry6905G8Ui5ONF6d5NFc365QG2EKcQVNli48fNazvM75nkMXJxrx3g0qv1ThuPJ8gNFUqqBb9sMkez7wSWZyErNjSK//7chK35c3VCgHfOHVZp2IeZHESQqGC17b1kpc1KBvaXSjgujSWN7mJTGPmPiMEt4EsTlrmvRvkJjSU/1GjbNDC1oIPTZWt+d+XxySyX3rFq/N3oR4VMjNiqkfMYMFPp+QNCjWFxhB6c5lcBmxRz42mWaeUa4wafVAke9riXu3NRWkPZHGS4yFhTlvhJy3XPbijKrtbzxYwTUYrnUmjMmg0Oo2YWx5TaVST3mg2mk16s0ahd/NkRibwew9w5wq6srIJWbxH4BvK9g1lj5jm2Sw1tjYa1AqzWmEy6S1ENDgALLaVQqPzhGyeiO4dyGJxuhVOI4v3LMS+DLGv68Uq3cHZr5s0OoXbzuoMl4DNpTFZ6B3dlXD20xL7MKsfuHBPVm2ZRuTVs1pBV8fZFveQMDl8mmNmxjsDGp3iG9zuvE0EAYHwnZvwsjg3q9r59XafS9l1MYMFNNSIuxTOXvWDUfdQ9+Mx2eAJPiJPOotL9NDcoLO0NBhufS8fmCwO6YPz5FKEo4FjcQCAvNZw81JTVbGGw6OrWom7XIXJohmNloAITv9RYkmYyycV6oFAs7gNg85KIfB2p1YAmCwC60M8C/gWRyAcCuriRZAcZHEEyUEWR5AcZHEEyUEWR5AcZHEEyfl/8jVd+6V/8yAAAAAASUVORK5CYII=",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLciWXq5f769"
      },
      "source": [
        "Awesome, now lets our agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew23XLqUf769",
        "outputId": "302173fa-7e98-4832-ecd9-7454ba8b691e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "How is the weather in Beijing on 1st of April 2025?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_weather_forecast (fdab2bac-0342-485e-9b10-f531ee3ddc27)\n",
            " Call ID: fdab2bac-0342-485e-9b10-f531ee3ddc27\n",
            "  Args:\n",
            "    date: 2025-04-01\n",
            "    location: Beijing\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_weather_forecast\n",
            "\n",
            "{'2025-04-01T00:00': 9.8, '2025-04-01T01:00': 14.3, '2025-04-01T02:00': 18.3, '2025-04-01T03:00': 19.6, '2025-04-01T04:00': 20.4, '2025-04-01T05:00': 20.8, '2025-04-01T06:00': 20.6, '2025-04-01T07:00': 20.8, '2025-04-01T08:00': 18.3, '2025-04-01T09:00': 18.1, '2025-04-01T10:00': 16.8, '2025-04-01T11:00': 15.0, '2025-04-01T12:00': 13.4, '2025-04-01T13:00': 12.2, '2025-04-01T14:00': 11.4, '2025-04-01T15:00': 10.7, '2025-04-01T16:00': 10.1, '2025-04-01T17:00': 9.5, '2025-04-01T18:00': 8.9, '2025-04-01T19:00': 8.2, '2025-04-01T20:00': 7.6, '2025-04-01T21:00': 7.2, '2025-04-01T22:00': 6.8, '2025-04-01T23:00': 7.5}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Okay, here is the weather forecast for Beijing on April 1st, 2025:\n",
            "\n",
            "The temperature will start at 9.8°C at midnight. It will rise quickly in the early morning, reaching a high of about 20.8°C between 5 AM and 7 AM. After that, the temperature is expected to decrease throughout the day, falling to around 13.4°C by noon and reaching a low of 7.5°C by 11 PM.\n"
          ]
        }
      ],
      "source": [
        "# Create our initial message dictionary\n",
        "inputs = {\"messages\": [(\"user\", \"How is the weather in Beijing on 1st of April 2025?\")]}\n",
        "\n",
        "# call our graph with streaming to see the steps\n",
        "\n",
        "for state in graph.stream(inputs, stream_mode=\"values\"):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    last_message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_9mOxu8f769"
      },
      "source": [
        "We can now continue with our conversation and for example ask for the weather in another city or let it compare it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWmIvQR2f769",
        "outputId": "7b027230-9a43-4e67-cb7c-95e6ce7ef0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Would it be in Vancouver warmer?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_weather_forecast (6d573b4b-ba73-464f-9b65-63ff0f0d0852)\n",
            " Call ID: 6d573b4b-ba73-464f-9b65-63ff0f0d0852\n",
            "  Args:\n",
            "    date: 2025-04-01\n",
            "    location: Vancouver\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_weather_forecast\n",
            "\n",
            "{'2025-04-01T00:00': 11.8, '2025-04-01T01:00': 11.1, '2025-04-01T02:00': 9.3, '2025-04-01T03:00': 8.2, '2025-04-01T04:00': 7.6, '2025-04-01T05:00': 7.2, '2025-04-01T06:00': 6.9, '2025-04-01T07:00': 6.5, '2025-04-01T08:00': 6.4, '2025-04-01T09:00': 6.5, '2025-04-01T10:00': 6.1, '2025-04-01T11:00': 5.7, '2025-04-01T12:00': 5.5, '2025-04-01T13:00': 5.3, '2025-04-01T14:00': 5.0, '2025-04-01T15:00': 5.2, '2025-04-01T16:00': 7.2, '2025-04-01T17:00': 8.6, '2025-04-01T18:00': 9.4, '2025-04-01T19:00': 9.8, '2025-04-01T20:00': 10.0, '2025-04-01T21:00': 9.9, '2025-04-01T22:00': 10.4, '2025-04-01T23:00': 10.7}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Based on the forecast for April 1st, 2025:\n",
            "\n",
            "*   **Beijing:** Temperatures will range from about 6.8°C to a high of 20.8°C.\n",
            "*   **Vancouver:** Temperatures will range from a low of 5.0°C to a high of 11.8°C.\n",
            "\n",
            "So, no, it will be significantly warmer in Beijing than in Vancouver on that day.\n"
          ]
        }
      ],
      "source": [
        "state[\"messages\"].append((\"user\", \"Would it be in Vancouver warmer?\"))\n",
        "\n",
        "\n",
        "for state in graph.stream(state, stream_mode=\"values\"):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    last_message.pretty_print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
