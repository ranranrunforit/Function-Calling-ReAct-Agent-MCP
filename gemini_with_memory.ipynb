{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### Mount Notebook to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbVK908ysZzL",
        "outputId": "faa213ec-10a8-47da-92fe-65c1ae02873b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change the working directory to the Drive root\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siNFlNEXsaeR",
        "outputId": "af296399-02a9-4b47-e137-c378b78c78cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet python-dotenv"
      ],
      "metadata": {
        "id": "G3UH4ZeUtAeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's do an import\n",
        "from dotenv import load_dotenv\n",
        "# Next it's time to load the API keys into environment variables\n",
        "# Always remember to do this!\n",
        "load_dotenv(override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH-kvd7Ss3GF",
        "outputId": "5b7a5354-19ce-4ac9-c802-e80c74636184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the keys\n",
        "\n",
        "import os\n",
        "gemini_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if gemini_api_key:\n",
        "    print(f\"Gemini API Key exists and begins {gemini_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"Gemini API Key not set - please head to the troubleshooting guide in the guides folder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvN9_WOYr5tZ",
        "outputId": "51dd85ba-22de-446e-ccc1-9d068c38d97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key exists and begins AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=gemini_api_key)\n",
        "\n",
        "messages = [\"What is 2+2?\"]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash-preview-05-20\", contents=messages\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cj116vdsNQG",
        "outputId": "d154e52e-c2db-4e1e-b98d-d9676379ce64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 + 2 = 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgJ1NXOFqrZX"
      },
      "source": [
        "# Integrating Long-Term Memory with Gemini 2.5\n",
        "\n",
        "By default, large language models (LLMs) are stateless, which means they do not remember past conversations. This can make it difficult to create truly personal and helpful AI applications. This guide shows you how to add long-term memory to your Gemini 2.5 chatbot using the Gemini API and [Mem0](https://github.com/mem0ai/mem0).\n",
        "\n",
        "By adding a memory system, your chatbot can:\n",
        "\n",
        "*   Remember details about the user from past conversations.\n",
        "*   Give answers that are more relevant and personal.\n",
        "*   Stop asking the same questions over and over.\n",
        "\n",
        "In this example, we will use `mem0`, an open-source tool for giving AI agents long-term memory and Gemini 2.5 Flash as the LLM. We will build a simple chatbot that saves what you talk about and uses that history to give you better, more personalized answers.\n",
        "\n",
        "## How does Mem0 work?\n",
        "\n",
        "Mem0 is designed to equip AI agents with scalable long-term memory, effectively addressing the limitations of fixed context windows in LLMs. At its core, mem0 works by reactively extracting, consolidating, and retrieving salient information from ongoing conversations.\n",
        "\n",
        "The process is split into four steps:\n",
        "\n",
        "1. Extract salient information from conversations using an LLM with dual context (a conversation summary combined with recent messages).\n",
        "2. Use LLM to process context and extract important new information and compares them against existing ones using semantic similarity.\n",
        "3. Update Memory (ADD, UPDATE, DELETE, or NOOP), for Mem0g variant (graph), extract entities and relationships.\n",
        "4. Use vector similarity search to fetch relevant memories for response generation.\n",
        "\n",
        "It uses vector embeddings to store and retrieve semantic information, maintaining user-specific context across sessions, and implementing efficient retrieval mechanisms for relevant past interactions.\n",
        "\n",
        "## Setup\n",
        "\n",
        "To get started, we need to install the Mem0 library and the Gemini API client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atqSsowpqrZc",
        "outputId": "4e399bf1-3809-489a-dfc9-a37f8f365293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m43 packages\u001b[0m \u001b[2min 686ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/126.75 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/737.38 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/737.38 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/737.38 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 14.87 KiB/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/737.38 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.79 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 14.87 KiB/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mbackoff   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.79 KiB/14.79 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 14.87 KiB/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 14.87 KiB/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 14.87 KiB/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 30.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 14.87 KiB/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 30.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 30.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 14.87 KiB/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 30.88 KiB/63.33 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 30.88 KiB/126.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 32.00 KiB/211.07 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 14.90 KiB/17.99 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 15.31 KiB/42.88 KiB\n",
            "\u001b[2mrequests  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 62.88 KiB/63.33 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 16.00 KiB/106.42 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 50.10 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 60.31 KiB/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 14.87 KiB/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 14.91 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 46.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[12A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mportalocker\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.99 KiB/17.99 KiB\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 34.89 KiB/42.88 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 32.00 KiB/106.42 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 78.01 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 60.73 KiB/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 30.87 KiB/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 30.91 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 46.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[11A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mtyping-extensions\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 34.89 KiB/42.88 KiB\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 32.00 KiB/106.42 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 78.01 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 60.73 KiB/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 30.87 KiB/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 30.91 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 62.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[10A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 57.45 KiB/106.42 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 94.01 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 92.73 KiB/211.07 KiB\n",
            "\u001b[2mgoogle-genai\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 78.53 KiB/220.77 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 141.26 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 62.88 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[9A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 73.45 KiB/106.42 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 126.01 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 124.31 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 194.40 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 112.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 78.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[8A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 89.45 KiB/106.42 KiB\n",
            "\u001b[2murllib3   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 126.75 KiB/126.75 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 124.31 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 194.40 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 112.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 78.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[8A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mposthog   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 89.45 KiB/106.42 KiB\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 32.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 124.31 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 205.26 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 112.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 78.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[7A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 48.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 172.31 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 253.26 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 144.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 94.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 48.00 KiB/170.75 KiB\n",
            "\u001b[2mgoogle-auth\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 188.31 KiB/211.07 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 253.26 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 144.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 94.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[6A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 64.00 KiB/170.75 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 285.26 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 160.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 126.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/13)\n",
            "\u001b[2mmem0ai    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 80.00 KiB/170.75 KiB\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 313.54 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 192.00 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 126.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[5A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/13)\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 313.54 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 319.89 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 252.90 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/13)\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 313.54 KiB/313.54 KiB\n",
            "\u001b[2mqdrant-client\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 319.89 KiB/321.26 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 252.90 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[4A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/13)\n",
            "\u001b[2mprotobuf  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 313.54 KiB/313.54 KiB\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 268.90 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/13)\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 286.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/13)\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 462.75 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/13)\n",
            "\u001b[2mopenai    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 606.65 KiB/737.38 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/13)\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (12/13)\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (12/13)\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (12/13)\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (12/13)\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (12/13)\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (12/13)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m13 packages\u001b[0m \u001b[2min 667ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m8 packages\u001b[0m \u001b[2min 143ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m13 packages\u001b[0m \u001b[2min 69ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbackoff\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.38.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.40.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.23.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.24.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmem0ai\u001b[0m\u001b[2m==0.1.114\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.93.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.93.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mportalocker\u001b[0m\u001b[2m==2.10.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mposthog\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.31.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqdrant-client\u001b[0m\u001b[2m==1.14.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.14.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install google-genai mem0ai --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS4owETYqrZd"
      },
      "source": [
        "## Memory initialization\n",
        "\n",
        "For building Memory we need to configure two main components:\n",
        "\n",
        "*   **LLM:** This model is responsible for processing the conversation, understanding the content, and extracting key information to be stored as memories.\n",
        "*   **Embedding Model:** This model takes the extracted text memories and converts them into numerical representations (vectors). This allows `mem0` to efficiently search and retrieve relevant memories based on their meaning when you ask a question.\n",
        "\n",
        "In this example, we will use Google's Gemini models for both tasks. We will use `gemini-2.5-flash` as our LLM and `text-embedding-004` as our embedding model. We are going to use a local Qdrant instance as our vector store. Mem0 supports multiple vector stores incuding MongoDB, OpenSearch, Elasticsearch, Pgvector and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXMDVXgtqrZd"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from mem0 import Memory\n",
        "\n",
        "# create client\n",
        "client = genai.Client()\n",
        "\n",
        "config = {\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"config\": {\n",
        "            \"model\": \"models/text-embedding-004\",\n",
        "        }\n",
        "    },\n",
        "        \"llm\": {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"config\": {\n",
        "            \"model\": \"gemini-2.5-flash\",\n",
        "            \"temperature\": 0.0,\n",
        "            \"max_tokens\": 2000,\n",
        "        }\n",
        "    },\n",
        "    \"vector_store\": {\n",
        "        \"config\": {\n",
        "            \"embedding_model_dims\": 768,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "memory = Memory.from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H6SZVqNqrZe"
      },
      "source": [
        "## Store memory about the user\n",
        "\n",
        "To simulate and simplify the process of adding memory. We will use dummy conversations between me (Philipp Schmid) and Gemini based on what can be seen on my blog. Conversations need to be in the `messages` format to be added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzDuQV8dqrZe",
        "outputId": "267e70ca-640a-4f9e-c9ea-1df53e3ccfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:mem0.memory.main:Invalid JSON response: Unterminated string starting at: line 60 column 21 (char 1858)\n"
          ]
        }
      ],
      "source": [
        "conv1 = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm planning a week-long vacation in early September. I want to go somewhere in Europe where I can do some serious mountain biking during the day and have a good selection of fiction and non-fiction books to read in the evenings. Any recommendations for destinations and book pairings?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"That sounds like a fantastic way to spend a week! Given your interest in mountain biking and reading, I'd recommend the Austrian Alps, specifically the area around Innsbruck. It's renowned for its challenging bike trails and stunning scenery. For your evenings, here are a few book pairings that might resonate with your interests:\\n\\n* Fiction: The Goldfinch by Donna Tartt. It's a long, immersive novel that would be perfect to get lost in after a long day of biking.\\n* Non-fiction: Endurance: Shackleton's Incredible Voyage by Alfred Lansing. It's a gripping true story of survival and leadership that might appeal to your adventurous spirit.\\n\\nI can also look for some local bookstores in Innsbruck that might have a good selection of English and German books if you'd like.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Innsbruck is a great suggestion! I've heard the trails there are epic. I've already read Endurance, but The Goldfinch sounds intriguing. Can you tell me a bit more about the plot without giving too much away? Also, are there any good sci-fi or tech-related non-fiction books you'd recommend?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Of course. The Goldfinch begins with a young boy who survives a terrorist bombing at an art museum. In the aftermath, he takes a famous painting, and the story follows his life as he navigates grief, love, and the criminal underworld, all while secretly holding onto the artwork. It's a beautifully written and complex story.\\n\\nFor a sci-fi recommendation, you might enjoy Project Hail Mary by Andy Weir. It's a thrilling story of a lone astronaut on a mission to save humanity, filled with clever problem-solving and scientific detail. As for tech non-fiction, given your professional background, you might find The Age of AI: And Our Human Future by Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher to be a thought-provoking read. It explores the societal implications of AI from a policy and philosophical perspective.\"}\n",
        "]\n",
        "result = memory.add(conv1, user_id=\"Chaoran\")\n",
        "\n",
        "conv2 =  [\n",
        "    {\"role\": \"user\", \"content\": \"I want to write a new post for my blog, but I want it to be more personal this time. I was thinking of drawing parallels between my experiences with mountain climbing and the challenges of developing and deploying large language models. What do you think of that idea?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"That's a fantastic idea for a blog post! It's a unique angle that could really resonate with your audience. You could structure it around a few key themes:\\n\\n1. The Ascent as the Training Process: You could compare the long, arduous process of training a large model with the physical and mental challenges of a difficult climb. Both require immense preparation, endurance, and the ability to push through plateaus.\\n2. Navigating Unseen Terrain as Dealing with Unexpected Model Behavior: Just as a climber might encounter unexpected obstacles on a route, developers often face surprising or undesirable model outputs. You could talk about the importance of having the right 'gear' (debugging tools, evaluation metrics) to navigate these challenges.\\n3. Reaching the Summit as Successful Deployment: The satisfaction of reaching a summit after a hard climb can be compared to the successful deployment of a model that solves a real-world problem. You could emphasize that this is not the end, but rather a new vantage point to see the next set of challenges.\\n\\nHow does that structure feel to you? We could also brainstorm some catchy titles.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love that structure! 'The Ascent as the Training Process' is a particularly strong analogy. For a title, I was thinking of something like 'Climbing the AI Mountain' or 'Lessons from the Summit: AI and the Art of the Climb.' What do you think?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Both are great titles! 'Lessons from the Summit: AI and the Art of the Climb' is particularly evocative and captures the reflective and personal tone you're aiming for. It suggests that you'll be sharing hard-won insights. You could even open the blog post with a short, vivid anecdote from one of your climbs to immediately draw the reader in.\"}\n",
        "]\n",
        "result = memory.add(conv2, user_id=\"Chaoran\")\n",
        "\n",
        "conv3 = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm preparing a talk for an upcoming AI conference. The topic is 'The Future of Agentic AI in the Enterprise'. I want to cover the current landscape, key challenges, and a forward-looking perspective. Can you help me brainstorm a compelling narrative and structure for the talk?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"That sounds like a fascinating topic! Here's a possible structure for your talk to create a strong narrative arc:\\n\\n1. The 'Why' Now: Start with a powerful hook. Perhaps a real-world example of a complex business problem that agentic AI can solve today. This sets the stage and grabs the audience's attention.\\n2. The Landscape of Agentic AI: Briefly cover the foundational concepts â€“ what makes an AI 'agentic'? Mention key frameworks and models that are driving this space. Given your background, you could draw comparisons between different approaches.\\n3. The 'How': Architecting for Success: This is where you can dive into the technical meat. Discuss design patterns for building robust and scalable agentic systems. You could touch upon topics like memory, tool use, and multi-agent collaboration.\\n4. The Hurdles to Overcome: Address the challenges head-on. This will make your talk more balanced and credible. Think about issues like reliability, security, and the 'black box' problem.\\n5. The Future, Today: Paint a picture of the near-future possibilities. What are the next big breakthroughs we can expect? You could end with a call to action for developers to start building and experimenting with agentic AI.\\n\\nHow does that sound as a starting point?\"},\n",
        "    {\"role\": \"user\", \"content\": \"This is a great starting point, thank you! I especially like the idea of starting with a strong 'Why Now'. For the 'Hurdles' section, I'm thinking of focusing on the challenges of evaluation and ensuring responsible AI development in agentic systems. Can you suggest some recent research papers or articles on evaluating agentic AI systems that I could reference?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Excellent addition. Focusing on evaluation and responsible AI is crucial. Here are a few highly-cited papers on evaluating agentic AI that you could explore:\\n\\n* 'A Survey on Evaluation of Large Language Models': While not exclusively on agents, it provides a comprehensive overview of evaluation metrics that are highly relevant.\\n* 'AgentBench: Evaluating LLMs as Agents': This paper introduces a benchmark specifically designed for evaluating LLMs as agents across a variety of tasks.\\n* 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ': A foundational paper on the ethical considerations of large language models, which is very relevant to responsible AI in agentic systems.\\n\\nI can also help you find more specific papers if you have a particular aspect of evaluation in mind.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Perfect, this is exactly what I was looking for. I'll start with these and let you know if I need more.\"}\n",
        "]\n",
        "result = memory.add(conv3, user_id=\"Chaoran\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUlqtV5xqrZf"
      },
      "source": [
        "We can manually search throught the memory to find relevant information. Let's try to find information about me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joYDmoApqrZf",
        "outputId": "34d44cb5-2764-4bec-ea0f-3c677b53fe02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wants to write a new personal blog post\n",
            "Is interested in \"Project Hail Mary\" (sci-fi)\n",
            "Wants to go somewhere in Europe\n"
          ]
        }
      ],
      "source": [
        "related_memories = memory.search(query=\"What do you know about me?\", user_id=\"Chaoran\")\n",
        "\n",
        "# Print the first 3 memories\n",
        "for m in related_memories[\"results\"][:3]:\n",
        "    print(m[\"memory\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g16Xi1FbqrZg"
      },
      "source": [
        "Great! Now let's do some test on how including memory into our conversation changes the output. Our memmory should now include that Philipp is going to give a talk about agentic AI in the enterprise and some context about my hobbies. P.S. I prefer weightlifting over mountain biking.\n",
        "\n",
        "Let see what Gemini will do when we ask it to help brainstorm about the upcoming talk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moaxHG4ZqrZg",
        "outputId": "fa8daf2f-a0ad-43d6-9198-57e9d085734e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a fantastic idea! Connecting your personal interests subtly makes your talk more memorable and authentic.\n",
            "\n",
            "To give you the best brainstorm, I need a little more information from you. **What are some of your personal interests or hobbies?**\n",
            "\n",
            "For example, do you enjoy:\n",
            "*   **Nature/Outdoors:** Hiking, gardening, astronomy, conservation?\n",
            "*   **Arts/Creativity:** Music, painting, writing, photography, film, cooking?\n",
            "*   **Games/Strategy:** Board games, video games, chess, sports?\n",
            "*   **History/Culture:** Archaeology, ancient civilizations, specific historical periods, languages?\n",
            "*   **Craftsmanship/Building:** Woodworking, coding, engineering, DIY?\n",
            "*   **Exploration/Discovery:** Travel, science, puzzles?\n",
            "\n",
            "Once I know your interests, I can tailor the suggestions.\n",
            "\n",
            "---\n",
            "\n",
            "In the meantime, here's a general framework and some initial ideas. I'll provide examples based on hypothetical interests to show you how it works:\n",
            "\n",
            "---\n",
            "\n",
            "## General Concepts for \"The Future of Agentic AI in the Enterprise\"\n",
            "\n",
            "### Titles (Focusing on different angles)\n",
            "\n",
            "1.  **The Transformative:**\n",
            "    *   \"The Autonomous Enterprise: Unleashing Agentic AI's Potential\"\n",
            "    *   \"Beyond Automation: The Rise of Agentic AI in Business\"\n",
            "    *   \"The Agentic Leap: Reshaping the Future of Work\"\n",
            "2.  **The Strategic/Action-Oriented:**\n",
            "    *   \"Navigating the Agentic Frontier: A Blueprint for Enterprise AI\"\n",
            "    *   \"Orchestrating Intelligence: Agentic AI for Strategic Advantage\"\n",
            "    *   \"The New Playbook: Winning with Agentic AI in the Enterprise\"\n",
            "3.  **The Evocative/Intriguing:**\n",
            "    *   \"The Living Enterprise: Where Agentic AI Thrives\"\n",
            "    *   \"Silent Partners, Powerful Results: The Era of Agentic AI\"\n",
            "    *   \"Beyond the Algorithm: Crafting the Agentic Future\"\n",
            "\n",
            "### Key Visuals (General Ideas)\n",
            "\n",
            "*   **Abstract Networks:** Glowing nodes connected by dynamic lines, forming complex, self-organizing patterns. Perhaps with a subtle corporate building outline in the background.\n",
            "*   **Flowing Data into Form:** Raw data streams coalescing into recognizable, intelligent forms (e.g., a digital brain, a complex gear system, or a simplified human outline).\n",
            "*   **The \"Conductor\":** A subtle, almost invisible hand (representing the agentic AI) subtly guiding complex business processes or data flows, rather than directly manipulating them.\n",
            "*   **Growth/Evolution:** A seed of light or data growing into a sophisticated, interconnected structure.\n",
            "\n",
            "---\n",
            "\n",
            "## How to Integrate Personal Interests (Examples)\n",
            "\n",
            "Let's assume a few hypothetical interests and brainstorm ideas:\n",
            "\n",
            "---\n",
            "\n",
            "### **Hypothetical Interest 1: Hiking & Nature Exploration**\n",
            "\n",
            "*   **Connection:** Paths, navigation, discovery, hidden beauty, mapping, resilience, reaching new summits, interconnected ecosystems.\n",
            "\n",
            "*   **Titles:**\n",
            "    *   \"**Summiting the Future: Agentic AI's Ascent in the Enterprise**\" (Subtle, aspirational)\n",
            "    *   \"**Blazing New Trails: Agentic AI's Expedition into Business Intelligence**\" (Suggests pioneering)\n",
            "    *   \"**The Ecosystem of Autonomy: Nurturing Agentic AI in Your Organization**\" (Focus on growth and interdependencies)\n",
            "\n",
            "*   **Key Visual:**\n",
            "    *   A stylized digital mountain range, with glowing data streams or circuit-like paths winding up to a futuristic, illuminated summit. At the peak, a subtle corporate logo or an abstract representation of an intelligent system.\n",
            "    *   Or, a close-up of moss/lichen on rocks, but the patterns within the moss are subtly circuit boards or data nodes.\n",
            "    *   A compass rose subtly integrated with AI circuitry, pointing towards a future enterprise.\n",
            "\n",
            "---\n",
            "\n",
            "### **Hypothetical Interest 2: Culinary Arts & Cooking**\n",
            "\n",
            "*   **Connection:** Recipes, ingredients, precision, transformation, fusion, orchestrating elements, \"mise en place,\" creating something new from components.\n",
            "\n",
            "*   **Titles:**\n",
            "    *   \"**The Agentic Kitchen: Crafting the Future of Enterprise Solutions**\" (Playful, emphasizes creation)\n",
            "    *   \"**Recipe for Autonomy: Orchestrating Agentic AI in Your Business**\" (Focus on process and control)\n",
            "    *   \"**Beyond the Ingredients: Cooking Up Agentic Intelligence for the Enterprise**\" (Highlights the outcome and new possibilities)\n",
            "\n",
            "*   **Key Visual:**\n",
            "    *   A futuristic chef's station where \"ingredients\" are data points and \"tools\" are lines of code or abstract AI symbols. A stylized chef's knife or whisk made of light, stirring a digital \"pot\" where complex systems are forming.\n",
            "    *   A visually appealing plate with \"dishes\" composed of abstract digital elements, subtly resembling gourmet food, perhaps with a glowing AI core.\n",
            "    *   A minimalist graphic showing a stack of \"ingredients\" (raw data, systems) being transformed into a beautifully plated, complex \"meal\" (the agentic solution) with a subtle glow.\n",
            "\n",
            "---\n",
            "\n",
            "### **Hypothetical Interest 3: Music & Composition**\n",
            "\n",
            "*   **Connection:** Harmony, rhythm, orchestration, improvisation, complexity from simple elements, performance, conducting, creating a unified sound.\n",
            "\n",
            "*   **Titles:**\n",
            "    *   \"**The Enterprise Orchestra: Composing the Future with Agentic AI**\" (Emphasizes collaboration and control)\n",
            "    *   \"**Symphony of Autonomy: Conducting Agentic AI in Your Business Strategy**\" (Suggests grand scale and leadership)\n",
            "    *   \"**Rhythm & Code: The Agentic Pulse of the Modern Enterprise**\" (Focus on dynamic, ongoing impact)\n",
            "\n",
            "*   **Key Visual:**\n",
            "    *   Musical staff lines transforming into data flows, with abstract notes representing AI agents working in harmony. A stylized conductor's baton made of light, directing these flows towards a cohesive enterprise output.\n",
            "    *   A visualizer of data forming intricate patterns like sound waves, perhaps with a subtle bass clef or treble clef integrated into the abstract design.\n",
            "    *   A futuristic DJ deck with glowing buttons and faders, but instead of audio waves, it's manipulating complex data streams that represent business processes.\n",
            "\n",
            "---\n",
            "\n",
            "## Your Turn!\n",
            "\n",
            "Please tell me some of your personal interests. The more specific, the better! With that, I can give you much more targeted and authentic ideas.\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are a helpful AI Assistant.\"\n",
        "\n",
        "prompt = \"Can you help me brainstorm a title and a key visual for my upcoming conference talk on 'The Future of Agentic AI in the Enterprise'? I'd like it to subtly connect to my personal interests in a way that feels authentic.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        "    config={\n",
        "        \"system_instruction\": system_prompt\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3mUCTqHqrZg"
      },
      "source": [
        "As expected, Gemini has no idea about the upcoming talk and my interests. Now lets add the memory to the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPHh0OA1qrZh",
        "outputId": "4366bcd8-596e-43b7-a2c0-7f31f150b2d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a fantastic idea to weave your personal interests into your professional talks! It makes them much more authentic and memorable.\n",
            "\n",
            "Given your interests in **mountain climbing**, **serious mountain biking**, and the book **\"Endurance: Shackleton's Incredible Voyage\"**, there's a strong thread of exploration, challenging journeys, resilience, and reaching new heights. This aligns perfectly with the complex and evolving nature of Agentic AI in the enterprise.\n",
            "\n",
            "Here are a few ideas, playing on those themes:\n",
            "\n",
            "---\n",
            "\n",
            "### Option 1: The \"Mountain Ascent\" Metaphor\n",
            "\n",
            "This directly connects to your blog post idea and your active interest in climbing/biking. Agentic AI involves building complex systems that can navigate challenging enterprise landscapes, much like an ascent.\n",
            "\n",
            "**Title Ideas:**\n",
            "1.  **\"The Agentic Ascent: Scaling Enterprise Intelligence\"**\n",
            "    *   *Why it works:* Directly uses the climbing metaphor (\"Ascent,\" \"Scaling\") and clearly states the topic. It implies progress, overcoming challenges, and reaching new capabilities.\n",
            "2.  **\"Beyond the Peak: Architecting Agentic AI for Enterprise Transformation\"**\n",
            "    *   *Why it works:* Suggests moving beyond current limitations, reaching a new vantage point (the \"peak\"), and then building the systems (\"architecting\") for future success.\n",
            "3.  **\"Pioneering the AI Frontier: Agentic Systems in the Enterprise\"**\n",
            "    *   *Why it works:* \"Frontier\" evokes exploration and discovery, fitting the adventurous spirit of your interests, while still being professional.\n",
            "\n",
            "**Key Visual Idea (for any of the above):**\n",
            "*   A sleek, abstract graphic of a mountain peak. The mountain itself could be subtly formed by interconnecting lines, nodes, or a simplified circuit board pattern, representing data, networks, or AI architecture.\n",
            "*   At the summit, a single, glowing node, a stylized beacon, or an abstract \"agent\" icon signifying successful enterprise outcomes, intelligent decision-making, or a new era of autonomy.\n",
            "*   The path leading up the mountain could be subtly highlighted, perhaps with small, incremental steps or a visual flow of data, suggesting the journey and challenges of deployment.\n",
            "*   **Subtle connection:** The overall aesthetic could be clean, modern, and aspirational, reflecting the effort and reward of a successful climb.\n",
            "\n",
            "---\n",
            "\n",
            "### Option 2: The \"Endurance & Navigation\" Metaphor\n",
            "\n",
            "This subtly nods to your interest in \"Endurance\" and the theme of resilience, strategic navigation, and overcoming extreme challenges inherent in pioneering new technologies like Agentic AI.\n",
            "\n",
            "**Title Ideas:**\n",
            "1.  **\"Enduring Intelligence: Building Resilient Agentic AI for the Enterprise\"**\n",
            "    *   *Why it works:* Directly references \"Endurance\" with \"Enduring Intelligence,\" implying AI systems that are robust, adaptable, and capable of navigating complex, long-term enterprise needs. \"Resilient\" reinforces this.\n",
            "2.  **\"Charting the Course: Navigating the Future of Agentic AI in the Enterprise\"**\n",
            "    *   *Why it works:* Focuses on strategic planning and guidance, like a skilled explorer navigating difficult terrain, which is crucial for successful AI deployment.\n",
            "\n",
            "**Key Visual Idea (for any of the above):**\n",
            "*   An abstract compass or navigational tool, perhaps with elements of a neural network or a stylized circuit board integrated into its design. The needle could be pointing towards a glowing \"future\" or \"solution\" point.\n",
            "*   Alternatively, a visual of a winding, illuminated path through a complex, interconnected digital landscape. The path itself could be composed of data streams, leading towards a clear, bright destination. The surrounding landscape would represent the intricate challenges, while the path signifies the resilient AI guiding the way.\n",
            "*   **Subtle connection:** The visual could use a color palette that evokes depth, foresight, and stability, suggesting the steadfastness required for a successful long-term journey.\n",
            "\n",
            "---\n",
            "\n",
            "**Recommendation:**\n",
            "\n",
            "I'd lean slightly towards **Option 1 (\"The Agentic Ascent\")** as it feels very direct to the \"future\" aspect of your talk and clearly leverages your mountain climbing interest, which you're already exploring for your blog post. The visual of climbing or scaling something implies progress and achievement, which is a powerful message for enterprise AI.\n",
            "\n",
            "Let me know what you think of these, and we can refine them further!\n"
          ]
        }
      ],
      "source": [
        "# retrieve memories\n",
        "helpful_memories = memory.search(query=prompt, user_id=\"Chaoran\")\n",
        "memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in helpful_memories[\"results\"])\n",
        "# extend system prompt\n",
        "extended_system_prompt = f\"You are a helpful AI Assistant. You have the following memories about the user:\\n{helpful_memories}\"\n",
        "\n",
        "# generate response\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        "    config={\n",
        "        \"system_instruction\": extended_system_prompt\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOu_CqSzqrZh"
      },
      "source": [
        "Awesome! Instead of needing to follow up with the user, Gemini generated two respones based on the memories, including the user's personal interests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q6HqcnRqrZh"
      },
      "source": [
        "## Long-term memory Chatbot with Gemini 2.5\n",
        "\n",
        "Now, lets combine all of this into an interactive chatbot. We separate the chatbot into two notebook cells, that we chat with Gemini, stop and start the chatbot again, but it has all the memories from the previous chat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdFbhju7qrZh"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from mem0 import Memory\n",
        "\n",
        "client = genai.Client()\n",
        "config = {\n",
        "    \"embedder\": {\"provider\": \"gemini\", \"config\": {\"model\": \"models/text-embedding-004\"}},\n",
        "    \"llm\": {\"provider\": \"gemini\", \"config\": {\"model\": \"gemini-2.5-flash\", \"temperature\": 0.0, \"max_tokens\": 2000}},\n",
        "    \"vector_store\": {\"config\": {\"embedding_model_dims\": 768}}\n",
        "}\n",
        "memory = Memory.from_config(config)\n",
        "\n",
        "system_prompt = \"You are a helpful AI. Answer the question based on query and memories.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XHs04d1qrZh"
      },
      "source": [
        "An example to test is you can start with saying something about you and where you are, e.g. \"I am live in Nuremberg and today we have 30Â°C, how can i cool down?\". Then stop (type 'exit'), restart (rerun the cell) the conversation and ask about \"what the closest swimming pool is\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ynY4XzZqrZh",
        "outputId": "0200cfab-0c04-4bff-dbcc-c87c4518b6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat with Gemini (type 'exit' to quit)\n",
            "You: what the closest swimming pool is\n",
            "what the closest swimming pool is\n",
            "Gemini: Since you live in **Nuremberg**, I can give you some of the most popular public swimming pools there. To find the *absolute closest* one, you'd need to put your specific address into a mapping app (like Google Maps) and search for \"Schwimmbad\" or \"Freibad\" (outdoor pool) from your current location.\n",
            "\n",
            "However, here are some of the main public swimming pools in Nuremberg, one of which is likely to be relatively close depending on where exactly you are:\n",
            "\n",
            "*   **Langwasserbad:** (Indoor and Outdoor pools) - One of the largest and most modern, with multiple pools, slides, and extensive outdoor areas.\n",
            "*   **SÃ¼dbad:** (Indoor and Outdoor pools) - Another major facility with various pools and a nice outdoor area for summer.\n",
            "*   **Nordostbad:** (Indoor pool) - A historic and charming indoor pool, great for laps and a more traditional swimming experience.\n",
            "*   **Westbad:** (Indoor and Outdoor pools) - Offers a good mix of indoor facilities and an outdoor area.\n",
            "*   **Katzwanger Bad:** (Outdoor pool) - A popular outdoor pool located in the Katzwang district, great for summer.\n",
            "\n",
            "I recommend using **Google Maps** or a similar navigation app, typing in your exact address, and then searching for \"Schwimmbad NÃ¼rnberg\" (swimming pool Nuremberg) or \"Freibad NÃ¼rnberg\" (outdoor pool Nuremberg) to get the most precise answer for your location.\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "def chat_with_memories(history: list[dict], user_id: str = \"default_user\") -> str:\n",
        "    # Retrieve relevant memories\n",
        "    print(history[-1][\"parts\"][0][\"text\"])\n",
        "    relevant_memories = memory.search(query=history[-1][\"parts\"][0][\"text\"], user_id=user_id, limit=5)\n",
        "    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n",
        "\n",
        "    # Generate Assistant response\n",
        "    memory_system_prompt = f\"{system_prompt}\\nUser Memories:\\n{memories_str}\"\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=history,\n",
        "        config={\"system_instruction\": memory_system_prompt}\n",
        "    )\n",
        "    history.append({\"role\": \"model\", \"parts\": [{\"text\": response.text}]})\n",
        "    # Create new memories from the conversation we need to convert the history to a list of messages\n",
        "    messages = [{\"role\": \"user\" if i % 2 == 0 else \"assistant\", \"content\": part[\"parts\"][0][\"text\"]} for i, part in enumerate(history)]\n",
        "    memory.add(messages, user_id=user_id)\n",
        "\n",
        "    return history\n",
        "\n",
        "def main():\n",
        "    print(\"Chat with Gemini (type 'exit' to quit)\")\n",
        "    history = []\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        history.append({\"role\": \"user\", \"parts\": [{\"text\": user_input}]})\n",
        "        response = chat_with_memories(history)\n",
        "        print(f\"Gemini: {response[-1]['parts'][0]['text']}\")\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWWiqgaIqrZi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}