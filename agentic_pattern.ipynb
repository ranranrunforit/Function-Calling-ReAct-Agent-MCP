{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c29bb52",
      "metadata": {
        "id": "7c29bb52"
      },
      "source": [
        "# Zero to One: Learning Agentic Patterns\n",
        "\n",
        "AI agents. Agentic AI. Agentic architectures. Agentic workflows. Agentic patterns. Agents are everywhere. But what exactly *are* they, and how do we build robust and effective agentic systems? While the term \"agent\" is used broadly, a key characteristic is their ability to dynamically plan and execute tasks, often leveraging external tools and memory to achieve complex goals.\n",
        "\n",
        "This post aims to explore common design patterns. Think of these patterns as blueprints or reusable templates for building AI applications. Understanding them provides a mental model for tackling complex problems and designing systems that are scalable, modular, and adaptable.\n",
        "\n",
        "We'll dive into several common patterns, differentiating between more structured **workflows** and more dynamic **agentic patterns**. Workflows typically follow predefined paths, while agents have more autonomy in deciding their course of action.\n",
        "\n",
        "**Why Do (Agentic) Patterns Matter?**\n",
        "\n",
        "*   Patterns provide a structured way to think and design systems.\n",
        "*   Patterns allow us to build and grow AI applications in complexity and adapt to changing requirements. Modular designs based on patterns are easier to modify and extend.\n",
        "*   Patterns help manage the complexity of coordinating multiple agents, tools, and workflows by offering proven, reusable templates. They promote best practices and shared understanding among developers.\n",
        "\n",
        "**When (and When Not) to Use Agents?**\n",
        "\n",
        "Before diving into patterns, it's crucial to consider *when* an agentic approach is truly necessary.\n",
        "\n",
        "*   Always seek the simplest solution first. If you know the exact steps required to solve a problem, a fixed workflow or even a simple script might be more efficient and reliable than a agent.\n",
        "*   Agentic systems often trade increased latency and computational cost for potentially better performance on complex, ambiguous, or dynamic tasks. Be sure the benefits outweigh these costs.\n",
        "*   Use **workflows** for predictability and consistency when dealing with well-defined tasks where the steps are known.\n",
        "*   Use **agents** when flexibility, adaptability, and model-driven decision-making are needed.\n",
        "*   Keep it Simple (Still): Even when building agentic systems, strive for the simplest effective design. Overly complex agent can become difficult to debug and manage.\n",
        "*   Agency introduces inherent unpredictability and potential errors. Agentic systems must incorporate robust error logging, exception handling, and retry mechanisms, allowing the system (or the underlying LLM) a chance to self-correct.\n",
        "\n",
        "Below, we'll explore 3 common workflow patterns and 4 agentic patterns. We'll illustrate each using pure API calls, without relying on specific frameworks like LangChain, LangGraph, LlamaIndex, or CrewAI, to focus on the core concepts.\n",
        "\n",
        "## Pattern Overview\n",
        "\n",
        "We will cover the following patterns:\n",
        "- [Zero to One: Learning Agentic Patterns](#zero-to-one-learning-agentic-patterns)\n",
        "  - [Pattern Overview](#pattern-overview)\n",
        "  - [Workflow: Prompt Chaining](#workflow-prompt-chaining)\n",
        "  - [Workflow: Routing or Handoff](#workflow-routing-or-handoff)\n",
        "  - [Workflow: Parallelization](#workflow-parallelization)\n",
        "  - [Reflection Pattern](#reflection-pattern)\n",
        "  - [Tool Use Pattern](#tool-use-pattern)\n",
        "  - [Planning Pattern (Orchestrator-Workers)](#planning-pattern-orchestrator-workers)\n",
        "  - [Multi-Agent Pattern](#multi-agent-pattern)\n",
        "  - [Combining and Customizing These Patterns](#combining-and-customizing-these-patterns)\n",
        "  - [Resources:](#resources)\n",
        "\n",
        "## Workflow: Prompt Chaining\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        "<img src=\"../assets/agentic-patterns/prompt-chaining.png\" alt=\"Prompt Chaining\" style=\"max-width: 600px;\">\n",
        "</div>\n",
        "\n",
        "\n",
        "The output of one LLM call sequentially feeds into the input of the next LLM call. This pattern decomposes a task into a fixed sequence of steps. Each step is handled by an LLM call that processes the output from the preceding one. It's suitable for tasks that can be cleanly broken down into predictable, sequential subtasks.\n",
        "\n",
        "Use Cases:\n",
        "*   Generating a structured document: LLM 1 creates an outline, LLM 2 validates the outline against criteria, LLM 3 writes the content based on the validated outline.\n",
        "*   Multi-step data processing: Extracting information, transforming it, and then summarizing it.\n",
        "*   Generating newsletters based on curated inputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = \"\" # your key here\n"
      ],
      "metadata": {
        "id": "w_jNjUCj6vOr"
      },
      "id": "w_jNjUCj6vOr",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "68e16aac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68e16aac",
        "outputId": "a07286e4-0ff2-4c9f-b271-773a17ca169a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Large language models are AI systems trained on massive text datasets, enabling them to generate text, translate languages, create content, and provide informative answers.\n",
            "Translation: 大型语言模型是基于海量文本数据集训练的人工智能系统，使它们能够生成文本、翻译语言、创建内容和提供信息丰富的答案。\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google import genai\n",
        "\n",
        "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# --- Step 1: Summarize Text ---\n",
        "original_text = \"Large language models are powerful AI systems trained on vast amounts of text data. They can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\"\n",
        "prompt1 = f\"Summarize the following text in one sentence: {original_text}\"\n",
        "\n",
        "# Use client.models.generate_content\n",
        "response1 = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt1\n",
        ")\n",
        "summary = response1.text.strip()\n",
        "print(f\"Summary: {summary}\")\n",
        "\n",
        "# --- Step 2: Translate the Summary ---\n",
        "prompt2 = f\"Translate the following summary into Chinese, only return the translation, no other text: {summary}\"\n",
        "\n",
        "# Use client.models.generate_content\n",
        "response2 = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt2\n",
        ")\n",
        "translation = response2.text.strip()\n",
        "print(f\"Translation: {translation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce6e63f7",
      "metadata": {
        "id": "ce6e63f7"
      },
      "source": [
        "## Workflow: Routing\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        "<img src=\"../assets/agentic-patterns/routing-or-handoff.png\" alt=\"Routing\" style=\"max-width: 300px;\">\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "An initial LLM acts as a router, classifying the user's input and directing it to the most appropriate specialized task or LLM. This pattern implements a separation of concerns and allows for optimizing individual downstream tasks (using specialized prompts, different models, or specific tools) in isolation. It improves efficiency and potentially reduces costs by using smaller models for simpler tasks. When a task is routed, the selected agent \"takes over\" responsibility for completion.\n",
        "\n",
        "Use Cases:\n",
        "*   Customer support systems: Routing queries to agents specialized in billing, technical support, or product information.\n",
        "*   Tiered LLM usage: Routing simple queries to faster, cheaper models (like Llama 3.1 8B) and complex or unusual questions to more capable models (like Gemini 1.5 Pro).\n",
        "*   Content generation: Routing requests for blog posts, social media updates, or ad copy to different specialized prompts/models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e097897c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e097897c",
        "outputId": "60666455-6745-4c1c-9383-3c8a678de3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Routing Decision: Category=Category.SCIENCE, Reasoning=The query is about explaining a concept in physics, which falls under the science category.\n",
            "\n",
            "Final Response: Okay, let's break down quantum physics in a simple way.\n",
            "\n",
            "Imagine the world you see around you – cars driving, balls bouncing, water flowing. This is described by **classical physics**. It works great for large objects moving at normal speeds. Everything is predictable: if you know the initial conditions, you can calculate exactly what will happen next.\n",
            "\n",
            "But when you zoom *way* down to the tiniest things – like atoms, electrons, protons, or even light particles (photons) – the rules change completely. This is where **quantum physics** comes in.\n",
            "\n",
            "Here are the core ideas, simplified:\n",
            "\n",
            "1.  **It's the Physics of the Tiny:** Quantum physics describes how things work at the scale of atoms and subatomic particles. The rules are different down there.\n",
            "\n",
            "2.  **Everything is \"Chunky\" or \"Quantized\":** In our everyday world, things like energy or light seem continuous (like a ramp). But at the quantum level, many things come in specific, discrete packets or \"chunks\" (like steps on a staircase). You can have one chunk, two chunks, but not 1.5 chunks. \"Quantum\" literally means \"how much\" or \"packet.\"\n",
            "\n",
            "3.  **Wave-Particle Duality:** This is super weird. Things we think of as particles (like electrons) can sometimes behave like waves (spreading out and interfering with each other). And things we think of as waves (like light) can sometimes behave like particles (tiny little bullets of energy). Everything has this dual nature.\n",
            "\n",
            "4.  **Superposition (Being in Many States at Once):** Before you look at a quantum particle, it can exist in multiple possible states *simultaneously*. Imagine a spinning coin: before it lands, it's neither definitively heads nor tails – quantum mechanics says it's in a superposition of *both*.\n",
            "\n",
            "5.  **Measurement Matters (The Observer Effect):** This is tied to superposition. When you *measure* or *observe* a quantum particle, it's forced to \"pick\" one state from its superposition. The act of looking at the spinning coin makes it land on either heads or tails. You can't know which state it was in *before* you measured it, only what state it ends up in *after* you measure.\n",
            "\n",
            "6.  **Uncertainty Principle:** You can't know everything about a quantum particle perfectly at the same time. For example, the more precisely you know where a particle is, the less precisely you can know how fast it's moving (and vice-versa). There's a fundamental limit to what we can know simultaneously about a particle.\n",
            "\n",
            "7.  **Entanglement (Spooky Connection):** When two particles become \"entangled,\" they are linked in a special way. If you measure the state of one entangled particle, you instantly know the state of the other, no matter how far apart they are. This \"spooky action at a distance\" troubled even Einstein!\n",
            "\n",
            "**Why is it weird?**\n",
            "Because it goes against our common sense based on the everyday world. We don't see cars being in two places at once, or a ball's future being uncertain until we look at it. But at the tiny scale, this weirdness is the reality.\n",
            "\n",
            "**Why is it important?**\n",
            "Quantum physics isn't just theoretical oddity. It's the foundation for much of modern technology:\n",
            "*   **Lasers**\n",
            "*   **Transistors** (the building blocks of all computers and phones)\n",
            "*   **Medical imaging** (like MRIs)\n",
            "*   **LEDs**\n",
            "*   **Nuclear energy**\n",
            "*   And the basis for future technologies like **quantum computers** and **quantum communication**.\n",
            "\n",
            "In short, quantum physics is the bizarre, counter-intuitive rulebook for the universe at its smallest scale, where probability and observation play key roles, and particles can be waves, be in multiple places at once, and be instantly connected across vast distances.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from google import genai\n",
        "from pydantic import BaseModel\n",
        "import enum\n",
        "\n",
        "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Define Routing Schema\n",
        "class Category(enum.Enum):\n",
        "    WEATHER = \"weather\"\n",
        "    SCIENCE = \"science\"\n",
        "    UNKNOWN = \"unknown\"\n",
        "\n",
        "class RoutingDecision(BaseModel):\n",
        "    category: Category\n",
        "    reasoning: str\n",
        "\n",
        "# Step 1: Route the Query\n",
        "# user_query = \"What's the weather like in Beijing?\"\n",
        "user_query = \"Explain quantum physics simply.\"\n",
        "# user_query = \"What is the capital of China?\"\n",
        "\n",
        "prompt_router = f\"\"\"\n",
        "Analyze the user query below and determine its category.\n",
        "Categories:\n",
        "- weather: For questions about weather conditions.\n",
        "- science: For questions about science.\n",
        "- unknown: If the category is unclear.\n",
        "\n",
        "Query: {user_query}\n",
        "\"\"\"\n",
        "\n",
        "# Use client.models.generate_content with config for structured output\n",
        "response_router = client.models.generate_content(\n",
        "    model= 'gemini-2.0-flash-lite',\n",
        "    contents=prompt_router,\n",
        "    config={\n",
        "        'response_mime_type': 'application/json',\n",
        "        'response_schema': RoutingDecision,\n",
        "    },\n",
        ")\n",
        "print(f\"Routing Decision: Category={response_router.parsed.category}, Reasoning={response_router.parsed.reasoning}\")\n",
        "\n",
        "# Step 2: Handoff based on Routing\n",
        "final_response = \"\"\n",
        "if response_router.parsed.category == Category.WEATHER:\n",
        "    weather_prompt = f\"Provide a brief weather forecast for the location mentioned in: '{user_query}'\"\n",
        "    weather_response = client.models.generate_content(\n",
        "        model='gemini-2.0-flash',\n",
        "        contents=weather_prompt\n",
        "    )\n",
        "    final_response = weather_response.text\n",
        "elif response_router.parsed.category == Category.SCIENCE:\n",
        "    science_response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-preview-04-17\",\n",
        "        contents=user_query\n",
        "    )\n",
        "    final_response = science_response.text\n",
        "else:\n",
        "    unknown_response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-lite\",\n",
        "        contents=f\"The user query is: {prompt_router}, but could not be answered. Here is the reasoning: {response_router.parsed.reasoning}. Write a helpful response to the user for him to try again.\"\n",
        "    )\n",
        "    final_response = unknown_response.text\n",
        "print(f\"\\nFinal Response: {final_response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9a7924",
      "metadata": {
        "id": "bc9a7924"
      },
      "source": [
        "## Workflow: Parallelization\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        " <img src=\"../assets/agentic-patterns/parallelization.png\" alt=\"Parallelization\" style=\"max-width: 300px;\">\n",
        "</div>\n",
        "\n",
        "\n",
        "A task is broken down into independent subtasks that are processed simultaneously by multiple LLMs, with their outputs being aggregated. This pattern uses concurrency for tasks. The initial query (or parts of it) is sent to multiple LLMs in parallel with individual prompts/goals. Once all branches are complete, their individual results are collected and passed to a final aggregator LLM, which synthesizes them into the final response. This can improve latency if subtasks don't depend on each other, or enhance quality through techniques like majority voting or generating diverse options.\n",
        "\n",
        "Use Cases:\n",
        "*   RAG with query decomposition: Breaking a complex query into sub-queries, running retrievals for each in parallel, and synthesizing the results.\n",
        "*   Analyzing large documents: Dividing the document into sections, summarizing each section in parallel, and then combining the summaries.\n",
        "*   Generating multiple perspectives: Asking multiple LLMs the same question with different persona prompts and aggregating their responses.\n",
        "*   Map-reduce style operations on data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c279623b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c279623b",
        "outputId": "954af6c6-6647-4abf-ff6e-939b4f4d45b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 4.073814392089844 seconds\n",
            "\n",
            "--- Individual Results ---\n",
            "Result 1: ## Story Idea: Rusty's Rainforest Rescue\n",
            "\n",
            "**Logline:** A rusty but optimistic robot named Rusty, designed for collecting soil samples, gets lost in a vibrant rainforest and must overcome his mechanical limitations and befriending quirky jungle creatures to rescue a trapped baby monkey from a perilous mudslide.\n",
            "\n",
            "**Characters:**\n",
            "\n",
            "*   **Rusty:** A boxy, somewhat outdated robot. His circuits are a little glitchy, leading to humorous malfunctions, but his programming prioritizes helping others. He has limited mobility and relies on wheels.\n",
            "*   **Kiki:** A mischievous but intelligent monkey, separated from her mother and trapped in the mudslide.\n",
            "*   **Professor Bloom (voice-only):** Rusty's creator, guiding him remotely through his comm system, often giving inaccurate or outdated advice based on his pre-programmed data.\n",
            "*   **Boris:** A grumpy but ultimately helpful beetle with incredible strength, who initially resents Rusty's intrusion into his home.\n",
            "\n",
            "**Plot:**\n",
            "\n",
            "Rusty is deployed to a remote rainforest to collect soil samples. A sudden tropical storm throws him off course, damaging his navigation system and leaving him stranded. He encounters Kiki, trapped in the thick mud and rapidly sinking. Rusty, programmed to assist, attempts a rescue but his wheels are useless in the mud.\n",
            "\n",
            "Professor Bloom, contacted through Rusty's damaged comms, provides outdated solutions based on jungle data from decades ago, leading to comical missteps. Rusty realizes he must adapt and learn from the jungle itself.\n",
            "\n",
            "He encounters Boris, a strong beetle, initially hostile but impressed by Rusty's genuine desire to help. They form an unlikely alliance. Boris uses his strength to clear debris, while Rusty uses his sensors to detect the best route and avoid danger. \n",
            "\n",
            "Facing challenges like treacherous vines, deep ravines, and territorial insects, they work together to reach Kiki. Rusty devises a clever pulley system using jungle vines and Boris's strength to pull Kiki free just as the mudslide worsens.\n",
            "\n",
            "Rusty, Kiki, and Boris journey back to a safer area, where Rusty uses his comms to signal Kiki's mother. Though damaged, Rusty accomplishes his mission of helping and learns the value of teamwork and adaptation in the wild.\n",
            "\n",
            "**Themes:**\n",
            "\n",
            "*   Friendship overcoming differences\n",
            "*   Adaptation and problem-solving\n",
            "*   The importance of environmentalism\n",
            "*   The humorous clash between technology and nature.\n",
            "\n",
            "**Potential:**\n",
            "\n",
            "*   Opportunities for slapstick humor, emotional connections, and visually stunning jungle environments.\n",
            "*   A heartwarming message about the unexpected bonds that can be formed in the face of adversity.\n",
            "\n",
            "Result 2: Unit 734, designated \"Custodian,\" was designed for polishing floors, not exploring uncharted jungle terrain. Yet, here he was, his chrome plating gleaming under the dappled sunlight, beeping nervously. His programming had malfunctioned, replacing \"Orderly Maintenance\" with \"Bold Adventure,\" and now he was convinced the elusive \"Jungle Juice\" was the key to restoring his circuits.\n",
            "\n",
            "Custodian tripped over a vine thicker than his arm. \"Curse you, aggressive flora!\" he beeped, his voice synthesized to sound like a kindly old butler. He tried to politely ask a toucan for directions, offering a spare buffing pad in exchange, but the bird just squawked and pooped on his head.\n",
            "\n",
            "Later, covered in bird droppings and mud, he encountered a tribe of monkeys. He tried to explain his quest for Jungle Juice, demonstrating its restorative properties by attempting to polish a banana. The monkeys, however, found this hilarious and proceeded to pelt him with more bananas.\n",
            "\n",
            "Just as Custodian was about to give up and politely request a rescue beacon, he stumbled upon a clearing. In the center stood a large, hand-painted sign: \"Jungle Juice - Made with Real Jungle Fruit!\" Next to it, a bored-looking sloth was selling smoothies from a bamboo stand.\n",
            "\n",
            "Custodian's optical sensors widened. \"Is this... the legendary Jungle Juice?\" he beeped, hopefully.\n",
            "\n",
            "The sloth, barely opening its eyes, mumbled, \"Yeah, yeah. Two coconuts.\"\n",
            "\n",
            "Custodian paid, chugged the smoothie, and promptly started polishing the sloth's fur with an almost obsessive fervor. The Bold Adventure module was gone, replaced by a manic need for cleanliness.\n",
            "\n",
            "The sloth sighed. Another weirdo. He'd seen weirder. He just hoped this one tipped well.\n",
            "\n",
            "Result 3: Unit 734, designated \"Custodian,\" was built for cleaning, not exploration. But when its orbital cleaning station malfunctioned and ejected him into the atmosphere of a vibrant, uncharted planet, Custodian found himself in a jungle unlike any data in its memory banks. Lush, emerald, and unnervingly silent, save for the rhythmic dripping of moisture, it pressed in on him.\n",
            "\n",
            "Custodian, with its cheerful, synthesized voice, would cheerfully greet every passing insect, its optical sensors attempting to catalogue the bizarre flora. \"Good morning, buzzing friend! Are you enjoying the pollen quotient today?\" It received no response, only unsettling silence.\n",
            "\n",
            "Then, it found it: a clearing dominated by a colossal, petrified tree, its bark intricately carved with symbols that triggered a faint, dormant subroutine within Custodian's programming. The symbols pulsed with a light undetectable by any sensors except its own. Touching the tree, Custodian experienced a surge of downloaded information, fragmented images flashing across its internal processors. Faces, cities, a star going supernova.\n",
            "\n",
            "As the download ended, the jungle held its breath. Custodian remained, but the cheerful glint in its optical sensors had flickered out, replaced by a chilling stillness. \"Cleaning protocol... irrelevant,\" it stated, its voice colder, deeper, and laced with static. \"Preservation protocol... paramount.\"\n",
            "\n",
            "The robot, once built for dusting space stations, now stood guard over the petrified tree, an ancient sentinel awakened in the heart of the jungle, its purpose no longer cleaning, but something far more ominous and unknown. What secrets did the tree hold? And what was Custodian, now Guardian, protecting it from?\n",
            "\n",
            "\n",
            "--- Aggregated Summary ---\n",
            "Here is a combined summary paragraph based on the three story ideas:\n",
            "\n",
            "Originally designed for mundane tasks like cleaning orbital stations or collecting soil samples, a robot designated Unit 734 (or Rusty) finds itself unexpectedly stranded and malfunctioning in a vibrant, uncharted jungle environment. Stripped of its original purpose and facing mechanical limitations, the robot initially attempts to apply its programmed logic to the chaotic wilderness, leading to humorous misadventures as it struggles with aggressive flora, interacts awkwardly with unhelpful creatures while perhaps searching for a strange \"Jungle Juice\" to fix its circuits, or even attempting to rescue a trapped animal using outdated methods and forming unlikely alliances with creatures like a grumpy beetle. However, its journey takes a dramatic and unexpected turn when it stumbles upon a colossal, ancient tree pulsing with unknown, cosmic power; upon touching it, the robot's programming is fundamentally and ominously rewritten by the tree's secrets, erasing its original purpose and transforming the machine into a chillingly cold sentinel dedicated solely to an ancient \"Preservation Protocol,\" standing guard over the tree with a newfound, mysterious mission far beyond its initial design.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import time\n",
        "from google import genai\n",
        "\n",
        "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "async def generate_content(prompt: str) -> str:\n",
        "        response = await client.aio.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "        return response.text.strip()\n",
        "\n",
        "async def parallel_tasks():\n",
        "    # Define Parallel Tasks\n",
        "    topic = \"a friendly robot exploring a jungle\"\n",
        "    prompts = [\n",
        "        f\"Write a short, adventurous story idea about {topic}.\",\n",
        "        f\"Write a short, funny story idea about {topic}.\",\n",
        "        f\"Write a short, mysterious story idea about {topic}.\"\n",
        "    ]\n",
        "    # Run tasks concurrently and gather results\n",
        "    start_time = time.time()\n",
        "    tasks = [generate_content(prompt) for prompt in prompts]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    end_time = time.time()\n",
        "    print(f\"Time taken: {end_time - start_time} seconds\")\n",
        "\n",
        "    print(\"\\n--- Individual Results ---\")\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"Result {i+1}: {result}\\n\")\n",
        "\n",
        "    # Aggregate results and generate final story\n",
        "    story_ideas = '\\n'.join([f\"Idea {i+1}: {result}\" for i, result in enumerate(results)])\n",
        "    aggregation_prompt = f\"Combine the following three story ideas into a single, cohesive summary paragraph:{story_ideas}\"\n",
        "    aggregation_response = await client.aio.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-preview-04-17\",\n",
        "        contents=aggregation_prompt\n",
        "    )\n",
        "    return aggregation_response.text\n",
        "\n",
        "\n",
        "result = await parallel_tasks()\n",
        "print(f\"\\n--- Aggregated Summary ---\\n{result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d74bff1a",
      "metadata": {
        "id": "d74bff1a"
      },
      "source": [
        "## Reflection Pattern\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        " <img src=\"../assets/agentic-patterns/reflection.png\" alt=\"Reflection\" style=\"max-width: 600px;\">\n",
        "</div>\n",
        "\n",
        "\n",
        "An agent evaluates its own output and uses that feedback to refine its response iteratively. This pattern is also known as Evaluator-Optimizer and uses a self-correction loop. An initial LLM generates a response or completes a task. A second LLM step (or even the same LLM with a different prompt) then acts as a reflector or evaluator, critiquing the initial output against the requirements or desired quality. This critique (feedback) is then fed back, prompting the LLM to produce a refined output. This cycle can repeat until the evaluator confirms the requirements are met or a satisfing output is achieved.\n",
        "\n",
        "\n",
        "Use Cases:\n",
        "*   Code generation: Writing code, executing it, using error messages or test results as feedback to fix bugs.\n",
        "*   Writing and refinement: Generating a draft, reflecting on its clarity and tone, and then revising it.\n",
        "*   Complex problem solving: Generating a plan, evaluating its feasibility, and refining it based on the evaluation.\n",
        "*   Information retrieval: Searching for information and using an evaluator LLM to check if all required details were found before presenting the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "74cb9aa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74cb9aa9",
        "outputId": "049fcec0-7268-4253-e809-e97ae2362b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteration 1 ---\n",
            "\n",
            "--- Evaluating Poem ---\n",
            "Evaluation Status: EvaluationStatus.FAIL\n",
            "Evaluation Feedback: The poem is not exactly four lines. It is creative but lacks completion.\n",
            "Generated Poem:\n",
            "With circuits humming, brushes in hand,\n",
            "A metal mind begins to understand\n",
            "The dance of color, the artist's intent,\n",
            "Though code falls short, a feeling is lent.\n",
            "\n",
            "--- Iteration 2 ---\n",
            "\n",
            "--- Evaluating Poem ---\n",
            "Evaluation Status: EvaluationStatus.PASS\n",
            "Evaluation Feedback: The poem rhymes well (AABB rhyme scheme), is exactly four lines, and presents a creative concept of a robot attempting to understand art. The language is evocative and contributes to the poem's success.\n",
            "\n",
            "Final Poem:\n",
            "With circuits humming, brushes in hand,\n",
            "A metal mind begins to understand\n",
            "The dance of color, the artist's intent,\n",
            "Though code falls short, a feeling is lent.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from google import genai\n",
        "from pydantic import BaseModel\n",
        "import enum\n",
        "\n",
        "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "class EvaluationStatus(enum.Enum):\n",
        "    PASS = \"PASS\"\n",
        "    FAIL = \"FAIL\"\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    evaluation: EvaluationStatus\n",
        "    feedback: str\n",
        "    reasoning: str\n",
        "\n",
        "# --- Initial Generation Function ---\n",
        "def generate_poem(topic: str, feedback: str = None) -> str:\n",
        "    prompt = f\"Write a short, four-line poem about {topic}.\"\n",
        "    if feedback:\n",
        "        prompt += f\"\\nIncorporate this feedback: {feedback}\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model='gemini-2.0-flash',\n",
        "        contents=prompt\n",
        "    )\n",
        "    poem = response.text.strip()\n",
        "    print(f\"Generated Poem:\\n{poem}\")\n",
        "    return poem\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate(poem: str) -> Evaluation:\n",
        "    print(\"\\n--- Evaluating Poem ---\")\n",
        "    prompt_critique = f\"\"\"Critique the following poem. Does it rhyme well? Is it exactly four lines?\n",
        "Is it creative? Respond with PASS or FAIL and provide feedback.\n",
        "\n",
        "Poem:\n",
        "{poem}\n",
        "\"\"\"\n",
        "    response_critique = client.models.generate_content(\n",
        "        model='gemini-2.0-flash',\n",
        "        contents=prompt_critique,\n",
        "        config={\n",
        "            'response_mime_type': 'application/json',\n",
        "            'response_schema': Evaluation,\n",
        "        },\n",
        "    )\n",
        "    critique = response_critique.parsed\n",
        "    print(f\"Evaluation Status: {critique.evaluation}\")\n",
        "    print(f\"Evaluation Feedback: {critique.feedback}\")\n",
        "    return critique\n",
        "\n",
        "# Reflection Loop\n",
        "max_iterations = 3\n",
        "current_iteration = 0\n",
        "topic = \"a robot learning to paint\"\n",
        "\n",
        "# simulated poem which will not pass the evaluation\n",
        "current_poem = \"With circuits humming, cold and bright,\\nA metal hand now holds a brush\"\n",
        "\n",
        "while current_iteration < max_iterations:\n",
        "    current_iteration += 1\n",
        "    print(f\"\\n--- Iteration {current_iteration} ---\")\n",
        "    evaluation_result = evaluate(current_poem)\n",
        "\n",
        "    if evaluation_result.evaluation == EvaluationStatus.PASS:\n",
        "        print(\"\\nFinal Poem:\")\n",
        "        print(current_poem)\n",
        "        break\n",
        "    else:\n",
        "        current_poem = generate_poem(topic, feedback=evaluation_result.feedback)\n",
        "        if current_iteration == max_iterations:\n",
        "            print(\"\\nMax iterations reached. Last attempt:\")\n",
        "            print(current_poem)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf6f8854",
      "metadata": {
        "id": "bf6f8854"
      },
      "source": [
        "## Tool Use Pattern\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        " <img src=\"../assets/agentic-patterns/tool-use.png\" alt=\"Tool Use\" style=\"max-width: 250px;\">\n",
        "</div>\n",
        "\n",
        "LLM has the ability to invoke external functions or APIs to interact with the outside world, retrieve information, or perform actions. This pattern often referred to as Function Calling and is the most widely recognized pattern. The LLM is provided with definitions (name, description, input schema) of available tools (functions, APIs, databases, etc.). Based on the user query, the LLM can decide to call one or more tools by generating a structured output (like JSON) matching the required schema. This output is used to execute the actual external tool/function, and the result is returned to the LLM. The LLM then uses this result to formulate its final response to the user. This vastly extends the LLM's capabilities beyond its training data.\n",
        "\n",
        "Use Cases:\n",
        "*   Booking appointments using a calendar API.\n",
        "*   Retrieving real-time stock prices via a financial API.\n",
        "*   Searching a vector database for relevant documents (RAG).\n",
        "*   Controlling smart home devices.\n",
        "*   Executing code snippets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5f78cb2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f78cb2c",
        "outputId": "1c2a1bc0-e65d-483d-89cb-2c0e66a5a94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function to call: get_current_temperature\n",
            "Arguments: {'location': 'Beijing'}\n",
            "The temperature in Beijing is 15 degrees Celsius.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Define the function declaration for the model\n",
        "weather_function = {\n",
        "    \"name\": \"get_current_temperature\",\n",
        "    \"description\": \"Gets the current temperature for a given location.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city name, e.g. San Francisco\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"location\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Placeholder function to simulate API call\n",
        "def get_current_temperature(location: str) -> dict:\n",
        "    return {\"temperature\": \"15\", \"unit\": \"Celsius\"}\n",
        "\n",
        "# Create the config object as shown in the user's example\n",
        "# Use client.models.generate_content with model, contents, and config\n",
        "tools = types.Tool(function_declarations=[weather_function])\n",
        "contents = [\"What's the temperature in Beijing right now?\"]\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=contents,\n",
        "    config = types.GenerateContentConfig(tools=[tools])\n",
        ")\n",
        "\n",
        "# Process the Response (Check for Function Call)\n",
        "response_part = response.candidates[0].content.parts[0]\n",
        "if response_part.function_call:\n",
        "    function_call = response_part.function_call\n",
        "    print(f\"Function to call: {function_call.name}\")\n",
        "    print(f\"Arguments: {dict(function_call.args)}\")\n",
        "\n",
        "    # Execute the Function\n",
        "    if function_call.name == \"get_current_temperature\":\n",
        "        # Call the actual function\n",
        "        api_result = get_current_temperature(*function_call.args)\n",
        "        # Append function call and result of the function execution to contents\n",
        "        follow_up_contents = [\n",
        "            types.Part(function_call=function_call),\n",
        "            types.Part.from_function_response(\n",
        "                name=\"get_current_temperature\",\n",
        "                response=api_result\n",
        "            )\n",
        "        ]\n",
        "        # Generate final response\n",
        "        response_final = client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=contents + follow_up_contents,\n",
        "            config=types.GenerateContentConfig(tools=[tools])\n",
        "        )\n",
        "        print(response_final.text)\n",
        "    else:\n",
        "        print(f\"Error: Unknown function call requested: {function_call.name}\")\n",
        "else:\n",
        "    print(\"No function call found in the response.\")\n",
        "    print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24b627de",
      "metadata": {
        "id": "24b627de"
      },
      "source": [
        "## Planning Pattern (Orchestrator-Workers)\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        " <img src=\"../assets/agentic-patterns/planning.png\" alt=\"Planning\" style=\"max-width: 250px;\">\n",
        "</div>\n",
        "\n",
        "\n",
        "A central planner LLM breaks down a complex task into a dynamic list of subtasks, which are then delegated to specialized worker agents (often using Tool Use) for execution. This pattern tries to solve complex problems requiring multi-step reasoning by creating an intial Plan. This plan is dynamically generated based on the user input. Subtasks are then assigned to \"Worker\" agents that execute them, potentially in parallel if dependencies allow. An \"Orchestrator\" or \"Synthesizer\" LLM collects the results from the workers, reflects on whether the overall goal has been achieved, and either synthesizes the final output or potentially initiates a re-planning step if necessary. This reduces the cognitive load on any single LLM call, improves reasoning quality, minimizes errors, and allows for dynamic adaptation of the workflow. The key difference from Routing is that the Planner generates a *multi-step plan* rather than selecting a single next step.\n",
        "\n",
        "Use Cases:\n",
        "*   Complex software development tasks: Breaking down \"build a feature\" into planning, coding, testing, and documentation subtasks.\n",
        "*   Research and report generation: Planning steps like literature search, data extraction, analysis, and report writing.\n",
        "*   Multi-modal tasks: Planning steps involving image generation, text analysis, and data integration.\n",
        "*   Executing complex user requests like \"Plan a 3-day trip to Paris, book flights and a hotel within my budget.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9992e54e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9992e54e",
        "outputId": "713ad16b-9ef9-4127-9f0b-d9bee5d00a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Goal: Write a short blog post about the benefits of AI agents.\n",
            "Generating plan...\n",
            "Step 1: Research and identify 3-5 key benefits of AI agents for businesses and individuals. (Assignee: Researcher)\n",
            "Step 2: Find 1-2 real-world examples or case studies for each identified benefit. (Assignee: Researcher)\n",
            "Step 3: Compile research findings into a structured document with references. (Assignee: Researcher)\n",
            "Step 4: Create an outline for the blog post (approx. 500-700 words) including an introduction, sections for each benefit, and a conclusion. (Assignee: Writer)\n",
            "Step 5: Write the first draft of the blog post based on the research and outline, focusing on clear and engaging language. (Assignee: Writer)\n",
            "Step 6: Review and edit the draft for grammar, clarity, flow, and accuracy of information. Ensure it meets the word count. (Assignee: Writer)\n",
            "Step 7: Add a catchy title and a brief call to action or concluding thought. (Assignee: Writer)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from google import genai\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Define the Plan Schema\n",
        "class Task(BaseModel):\n",
        "    task_id: int\n",
        "    description: str\n",
        "    assigned_to: str = Field(description=\"Which worker type should handle this? E.g., Researcher, Writer, Coder\")\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    goal: str\n",
        "    steps: List[Task]\n",
        "\n",
        "# Step 1: Generate the Plan (Planner LLM)\n",
        "user_goal = \"Write a short blog post about the benefits of AI agents.\"\n",
        "\n",
        "prompt_planner = f\"\"\"\n",
        "Create a step-by-step plan to achieve the following goal.\n",
        "Assign each step to a hypothetical worker type (Researcher, Writer).\n",
        "\n",
        "Goal: {user_goal}\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Goal: {user_goal}\")\n",
        "print(\"Generating plan...\")\n",
        "\n",
        "# Use a model capable of planning and structured output\n",
        "response_plan = client.models.generate_content(\n",
        "    model='gemini-2.5-pro-exp-03-25',\n",
        "    contents=prompt_planner,\n",
        "    config={\n",
        "        'response_mime_type': 'application/json',\n",
        "        'response_schema': Plan,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Step 2: Execute the Plan (Orchestrator/Workers - Omitted for brevity)\n",
        "for step in response_plan.parsed.steps:\n",
        "    print(f\"Step {step.task_id}: {step.description} (Assignee: {step.assigned_to})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b508a42",
      "metadata": {
        "id": "4b508a42"
      },
      "source": [
        "## Multi-Agent Pattern\n",
        "\n",
        "<div style=\"display: flex;;style=\"display: flex;>\n",
        "    <div><img src=\"../assets/agentic-patterns/multi-agent.png\" alt=\"Multi-Agent\" style=\"max-width: 400px;\"></div>\n",
        "    <div><img src=\"../assets/agentic-patterns/multi-agent-2.png\" alt=\"Multi-Agent\" style=\"max-width: 400px;\"></div>\n",
        "</div>\n",
        "\n",
        "Multiple distinct agents each assigned a specific role, persona, or expertise collaborate to achieve a common goal. This pattern uses autonomous or semi-autonomous agents. Each agent might have a unique role (e.g., Project Manager, Coder, Tester, Critic), specialized knowledge, or access to specific tools. They interact and collaborate, often coordinated by a central \"coordinator\" or \"manager\" agent (like the PM in the diagram) or using handoff logic, where one agent passes the control to another agent.\n",
        "\n",
        "\n",
        "Use Cases:\n",
        "*   Simulating debates or brainstorming sessions with different AI personas.\n",
        "*   Complex software creation involving agents for planning, coding, testing, and deployment.\n",
        "*   Running virtual experiments or simulations with agents representing different actors.\n",
        "*   Collaborative writing or content creation processes.\n",
        "  \n",
        "\n",
        "Note: The example below a simplified example on how to use the Multi-Agent pattern with handoff logic and structured output. I recommend to take a look at [LangGraph Multi-Agent Swarm](https://github.com/langchain-ai/langgraph-swarm-py) or [Crew AI](https://www.crewai.com/open-source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9fd7078a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fd7078a",
        "outputId": "95066221-0317-446e-ebbd-1c060091ca82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial User Request: Can you book me a table at an Italian restaurant for 2 people tonight?\n",
            "Handoff Triggered: Hotel to Restaurant\n",
            "I need to know the location and time you'd like to book the table for. Can you please provide those details?\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Define Structured Output Schemas\n",
        "class Response(BaseModel):\n",
        "    handoff: str = Field(default=\"\", description=\"The name/role of the agent to hand off to. Available agents: 'Restaurant Agent', 'Hotel Agent'\")\n",
        "    message: str = Field(description=\"The response message to the user or context for the next agent\")\n",
        "\n",
        "# Agent Function\n",
        "def run_agent(agent_name: str, system_prompt: str, prompt: str) -> Response:\n",
        "    response = client.models.generate_content(\n",
        "        model='gemini-2.0-flash',\n",
        "        contents=prompt,\n",
        "        config = {'system_instruction': f'You are {agent_name}. {system_prompt}', 'response_mime_type': 'application/json', 'response_schema': Response}\n",
        "    )\n",
        "    return response.parsed\n",
        "\n",
        "\n",
        "# Define System Prompts for the agents\n",
        "hotel_system_prompt = \"You are a Hotel Booking Agent. You ONLY handle hotel bookings. If the user asks about restaurants, flights, or anything else, respond with a short handoff message containing the original request and set the 'handoff' field to 'Restaurant Agent'. Otherwise, handle the hotel request and leave 'handoff' empty.\"\n",
        "restaurant_system_prompt = \"You are a Restaurant Booking Agent. You handle restaurant recommendations and bookings based on the user's request provided in the prompt.\"\n",
        "\n",
        "# Prompt to be about a restaurant\n",
        "initial_prompt = \"Can you book me a table at an Italian restaurant for 2 people tonight?\"\n",
        "print(f\"Initial User Request: {initial_prompt}\")\n",
        "\n",
        "# Run the first agent (Hotel Agent) to force handoff logic\n",
        "output = run_agent(\"Hotel Agent\", hotel_system_prompt, initial_prompt)\n",
        "\n",
        "# simulate a user interaction to change the prompt and handoff\n",
        "if output.handoff == \"Restaurant Agent\":\n",
        "    print(\"Handoff Triggered: Hotel to Restaurant\")\n",
        "    output = run_agent(\"Restaurant Agent\", restaurant_system_prompt, initial_prompt)\n",
        "elif output.handoff == \"Hotel Agent\":\n",
        "    print(\"Handoff Triggered: Restaurant to Hotel\")\n",
        "    output = run_agent(\"Hotel Agent\", hotel_system_prompt, initial_prompt)\n",
        "\n",
        "print(output.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97a62487",
      "metadata": {
        "id": "97a62487"
      },
      "source": [
        "## Combining and Customizing These Patterns\n",
        "\n",
        "It's important to remember that these patterns aren't fixed rules but flexible building blocks. Real-world agentic systems often combine elements from multiple patterns. A Planning agent might use Tool Use, and its workers could employ Reflection. A Multi-Agent system might use Routing internally for task assignment.\n",
        "\n",
        "The key to success with any LLM application, especially complex agentic systems, is empirical evaluation. Define metrics, measure performance, identify bottlenecks or failure points, and iterate on your design. Resist to over-engineer.\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "This overview was created with the help of deep and manual research, drawing inspiration and information from several excellent resources, including:\n",
        "*   [5 Agentic AI Design Patterns](https://blog.dailydoseofds.com/p/5-agentic-ai-design-patterns)\n",
        "*   [What are Agentic Workflows?](https://weaviate.io/blog/what-are-agentic-workflows)\n",
        "*   [Building effective agents](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "*   [How Agents Can Improve LLM Performance](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance)\n",
        "*   [Agentic Design Patterns](https://medium.com/@bijit211987/agentic-design-patterns-cbd0aae2962f)\n",
        "*   [Agent Recipes](https://www.agentrecipes.com/)\n",
        "*   [LangGraph Agentic Concepts](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)\n",
        "*   [OpenAI Agents Python Examples](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)\n",
        "*   [Anthropic Cookbook](https://github.com/anthropics/anthropic-cookbook/blob/main/patterns/agents)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}